<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />

<meta name="viewport" content="width=device-width, initial-scale=1" />


<meta name="date" content="2021-06-03" />

<title>DoubleML - An Object-Oriented Implementation of Double Machine Learning in R</title>

<script src="data:application/javascript;base64,Ly8gUGFuZG9jIDIuOSBhZGRzIGF0dHJpYnV0ZXMgb24gYm90aCBoZWFkZXIgYW5kIGRpdi4gV2UgcmVtb3ZlIHRoZSBmb3JtZXIgKHRvCi8vIGJlIGNvbXBhdGlibGUgd2l0aCB0aGUgYmVoYXZpb3Igb2YgUGFuZG9jIDwgMi44KS4KZG9jdW1lbnQuYWRkRXZlbnRMaXN0ZW5lcignRE9NQ29udGVudExvYWRlZCcsIGZ1bmN0aW9uKGUpIHsKICB2YXIgaHMgPSBkb2N1bWVudC5xdWVyeVNlbGVjdG9yQWxsKCJkaXYuc2VjdGlvbltjbGFzcyo9J2xldmVsJ10gPiA6Zmlyc3QtY2hpbGQiKTsKICB2YXIgaSwgaCwgYTsKICBmb3IgKGkgPSAwOyBpIDwgaHMubGVuZ3RoOyBpKyspIHsKICAgIGggPSBoc1tpXTsKICAgIGlmICghL15oWzEtNl0kL2kudGVzdChoLnRhZ05hbWUpKSBjb250aW51ZTsgIC8vIGl0IHNob3VsZCBiZSBhIGhlYWRlciBoMS1oNgogICAgYSA9IGguYXR0cmlidXRlczsKICAgIHdoaWxlIChhLmxlbmd0aCA+IDApIGgucmVtb3ZlQXR0cmlidXRlKGFbMF0ubmFtZSk7CiAgfQp9KTsK"></script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>


<style type="text/css">
  code {
    white-space: pre;
  }
  .sourceCode {
    overflow: visible;
  }
</style>
<style type="text/css" data-origin="pandoc">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */

</style>
<script>
// apply pandoc div.sourceCode style to pre.sourceCode instead
(function() {
  var sheets = document.styleSheets;
  for (var i = 0; i < sheets.length; i++) {
    if (sheets[i].ownerNode.dataset["origin"] !== "pandoc") continue;
    try { var rules = sheets[i].cssRules; } catch (e) { continue; }
    for (var j = 0; j < rules.length; j++) {
      var rule = rules[j];
      // check if there is a div.sourceCode rule
      if (rule.type !== rule.STYLE_RULE || rule.selectorText !== "div.sourceCode") continue;
      var style = rule.style.cssText;
      // check if color or background-color is set
      if (rule.style.color === '' && rule.style.backgroundColor === '') continue;
      // replace div.sourceCode by a pre.sourceCode rule
      sheets[i].deleteRule(j);
      sheets[i].insertRule('pre.sourceCode{' + style + '}', j);
    }
  }
})();
</script>

<style type="text/css">
  p.abstract{
    text-align: center;
    font-weight: bold;
  }
  div.abstract{
    margin: auto;
    width: 90%;
  }
</style>


<style type="text/css">
/* for pandoc --citeproc since 2.11 */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="data:text/css,body%20%7B%0Abackground%2Dcolor%3A%20%23fff%3B%0Amargin%3A%201em%20auto%3B%0Amax%2Dwidth%3A%20700px%3B%0Aoverflow%3A%20visible%3B%0Apadding%2Dleft%3A%202em%3B%0Apadding%2Dright%3A%202em%3B%0Afont%2Dfamily%3A%20%22Open%20Sans%22%2C%20%22Helvetica%20Neue%22%2C%20Helvetica%2C%20Arial%2C%20sans%2Dserif%3B%0Afont%2Dsize%3A%2014px%3B%0Aline%2Dheight%3A%201%2E35%3B%0A%7D%0A%23TOC%20%7B%0Aclear%3A%20both%3B%0Amargin%3A%200%200%2010px%2010px%3B%0Apadding%3A%204px%3B%0Awidth%3A%20400px%3B%0Aborder%3A%201px%20solid%20%23CCCCCC%3B%0Aborder%2Dradius%3A%205px%3B%0Abackground%2Dcolor%3A%20%23f6f6f6%3B%0Afont%2Dsize%3A%2013px%3B%0Aline%2Dheight%3A%201%2E3%3B%0A%7D%0A%23TOC%20%2Etoctitle%20%7B%0Afont%2Dweight%3A%20bold%3B%0Afont%2Dsize%3A%2015px%3B%0Amargin%2Dleft%3A%205px%3B%0A%7D%0A%23TOC%20ul%20%7B%0Apadding%2Dleft%3A%2040px%3B%0Amargin%2Dleft%3A%20%2D1%2E5em%3B%0Amargin%2Dtop%3A%205px%3B%0Amargin%2Dbottom%3A%205px%3B%0A%7D%0A%23TOC%20ul%20ul%20%7B%0Amargin%2Dleft%3A%20%2D2em%3B%0A%7D%0A%23TOC%20li%20%7B%0Aline%2Dheight%3A%2016px%3B%0A%7D%0Atable%20%7B%0Amargin%3A%201em%20auto%3B%0Aborder%2Dwidth%3A%201px%3B%0Aborder%2Dcolor%3A%20%23DDDDDD%3B%0Aborder%2Dstyle%3A%20outset%3B%0Aborder%2Dcollapse%3A%20collapse%3B%0A%7D%0Atable%20th%20%7B%0Aborder%2Dwidth%3A%202px%3B%0Apadding%3A%205px%3B%0Aborder%2Dstyle%3A%20inset%3B%0A%7D%0Atable%20td%20%7B%0Aborder%2Dwidth%3A%201px%3B%0Aborder%2Dstyle%3A%20inset%3B%0Aline%2Dheight%3A%2018px%3B%0Apadding%3A%205px%205px%3B%0A%7D%0Atable%2C%20table%20th%2C%20table%20td%20%7B%0Aborder%2Dleft%2Dstyle%3A%20none%3B%0Aborder%2Dright%2Dstyle%3A%20none%3B%0A%7D%0Atable%20thead%2C%20table%20tr%2Eeven%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0A%7D%0Ap%20%7B%0Amargin%3A%200%2E5em%200%3B%0A%7D%0Ablockquote%20%7B%0Abackground%2Dcolor%3A%20%23f6f6f6%3B%0Apadding%3A%200%2E25em%200%2E75em%3B%0A%7D%0Ahr%20%7B%0Aborder%2Dstyle%3A%20solid%3B%0Aborder%3A%20none%3B%0Aborder%2Dtop%3A%201px%20solid%20%23777%3B%0Amargin%3A%2028px%200%3B%0A%7D%0Adl%20%7B%0Amargin%2Dleft%3A%200%3B%0A%7D%0Adl%20dd%20%7B%0Amargin%2Dbottom%3A%2013px%3B%0Amargin%2Dleft%3A%2013px%3B%0A%7D%0Adl%20dt%20%7B%0Afont%2Dweight%3A%20bold%3B%0A%7D%0Aul%20%7B%0Amargin%2Dtop%3A%200%3B%0A%7D%0Aul%20li%20%7B%0Alist%2Dstyle%3A%20circle%20outside%3B%0A%7D%0Aul%20ul%20%7B%0Amargin%2Dbottom%3A%200%3B%0A%7D%0Apre%2C%20code%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0Aborder%2Dradius%3A%203px%3B%0Acolor%3A%20%23333%3B%0Awhite%2Dspace%3A%20pre%2Dwrap%3B%20%0A%7D%0Apre%20%7B%0Aborder%2Dradius%3A%203px%3B%0Amargin%3A%205px%200px%2010px%200px%3B%0Apadding%3A%2010px%3B%0A%7D%0Apre%3Anot%28%5Bclass%5D%29%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0A%7D%0Acode%20%7B%0Afont%2Dfamily%3A%20Consolas%2C%20Monaco%2C%20%27Courier%20New%27%2C%20monospace%3B%0Afont%2Dsize%3A%2085%25%3B%0A%7D%0Ap%20%3E%20code%2C%20li%20%3E%20code%20%7B%0Apadding%3A%202px%200px%3B%0A%7D%0Adiv%2Efigure%20%7B%0Atext%2Dalign%3A%20center%3B%0A%7D%0Aimg%20%7B%0Abackground%2Dcolor%3A%20%23FFFFFF%3B%0Apadding%3A%202px%3B%0Aborder%3A%201px%20solid%20%23DDDDDD%3B%0Aborder%2Dradius%3A%203px%3B%0Aborder%3A%201px%20solid%20%23CCCCCC%3B%0Amargin%3A%200%205px%3B%0A%7D%0Ah1%20%7B%0Amargin%2Dtop%3A%200%3B%0Afont%2Dsize%3A%2035px%3B%0Aline%2Dheight%3A%2040px%3B%0A%7D%0Ah2%20%7B%0Aborder%2Dbottom%3A%204px%20solid%20%23f7f7f7%3B%0Apadding%2Dtop%3A%2010px%3B%0Apadding%2Dbottom%3A%202px%3B%0Afont%2Dsize%3A%20145%25%3B%0A%7D%0Ah3%20%7B%0Aborder%2Dbottom%3A%202px%20solid%20%23f7f7f7%3B%0Apadding%2Dtop%3A%2010px%3B%0Afont%2Dsize%3A%20120%25%3B%0A%7D%0Ah4%20%7B%0Aborder%2Dbottom%3A%201px%20solid%20%23f7f7f7%3B%0Amargin%2Dleft%3A%208px%3B%0Afont%2Dsize%3A%20105%25%3B%0A%7D%0Ah5%2C%20h6%20%7B%0Aborder%2Dbottom%3A%201px%20solid%20%23ccc%3B%0Afont%2Dsize%3A%20105%25%3B%0A%7D%0Aa%20%7B%0Acolor%3A%20%230033dd%3B%0Atext%2Ddecoration%3A%20none%3B%0A%7D%0Aa%3Ahover%20%7B%0Acolor%3A%20%236666ff%3B%20%7D%0Aa%3Avisited%20%7B%0Acolor%3A%20%23800080%3B%20%7D%0Aa%3Avisited%3Ahover%20%7B%0Acolor%3A%20%23BB00BB%3B%20%7D%0Aa%5Bhref%5E%3D%22http%3A%22%5D%20%7B%0Atext%2Ddecoration%3A%20underline%3B%20%7D%0Aa%5Bhref%5E%3D%22https%3A%22%5D%20%7B%0Atext%2Ddecoration%3A%20underline%3B%20%7D%0A%0Acode%20%3E%20span%2Ekw%20%7B%20color%3A%20%23555%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%0Acode%20%3E%20span%2Edt%20%7B%20color%3A%20%23902000%3B%20%7D%20%0Acode%20%3E%20span%2Edv%20%7B%20color%3A%20%2340a070%3B%20%7D%20%0Acode%20%3E%20span%2Ebn%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Efl%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Ech%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Est%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Eco%20%7B%20color%3A%20%23888888%3B%20font%2Dstyle%3A%20italic%3B%20%7D%20%0Acode%20%3E%20span%2Eot%20%7B%20color%3A%20%23007020%3B%20%7D%20%0Acode%20%3E%20span%2Eal%20%7B%20color%3A%20%23ff0000%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%0Acode%20%3E%20span%2Efu%20%7B%20color%3A%20%23900%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%0Acode%20%3E%20span%2Eer%20%7B%20color%3A%20%23a61717%3B%20background%2Dcolor%3A%20%23e3d2d2%3B%20%7D%20%0A" type="text/css" />




</head>

<body>




<h1 class="title toc-ignore">DoubleML - An Object-Oriented Implementation of Double Machine Learning in R</h1>
<h4 class="date">2021-06-03</h4>
<div class="abstract">
<p class="abstract">Abstract</p>
The R package <code>DoubleML</code> implements the double/debiased machine learning framework of Chernozhukov et al. (2018). It provides functionalities to estimate parameters in causal models based on machine learning methods. The double machine learning framework consist of three key ingredients Neyman orthogonality, high-quality machine learning estimation and sample splitting. Estimation of nuisance components can be performed by various state-of-the-art machine learning methods that are available in the <code>mlr3</code> ecosystem. <code>DoubleML</code> makes it possible to perform inference in a variety of causal models, including partially linear and interactive regression models and their extensions to instrumental variable estimation. The object-oriented implementation of <code>DoubleML</code> enables a high flexibility for the model specification and makes it easily extendable. This paper serves as an introduction to the double machine learning framework and the R package <code>DoubleML</code>. In reproducible code examples with simulated and real data sets, we demonstrate how <code>DoubleML</code> users can perform valid inference based on machine learning methods.
</div>



<!-- Uncomment meta-info from previous template (ASA) -->
<!-- - name: Philipp Bach -->
<!--   affiliation: University of Hamburg, Hamburg Business School -->
<!-- - name: Malte S. Kurz -->
<!--   affiliation: University of Hamburg, Hamburg Business School -->
<!-- - name: Victor Chernozhukov -->
<!--   affiliation: MIT -->
<!-- - name: Martin Spindler -->
<!--   affiliation: University of Hamburg, Hamburg Business School -->
<!-- \newtheorem{Remark}{Remark} -->
<p>Structural equation models provide a quintessential framework for conducting causal inference in statistics, econometrics, machine learning (ML), and other data sciences. The package <code>DoubleML</code> for R <span class="citation">(R Core Team 2020)</span> implements partially linear and interactive structural equation and treatment effect models with high-dimensional confounding variables as considered in <span class="citation">Chernozhukov et al. (2018)</span>. Estimation and tuning of the machine learning models is based on the powerful functionalities provided by the <code>mlr3</code> package and the <code>mlr3</code> ecosystem <span class="citation">(Lang et al. 2019)</span>. A key element of double machine learning (DML) models are score functions identifying the estimates for the target parameter. These functions play an essential role for valid inference with machine learning methods because they have to satisfy a property called Neyman orthogonality. With the score functions as key elements, <code>DoubleML</code> implements double machine learning in a very general way using object orientation based on the <code>R6</code> package <span class="citation">(Chang 2020)</span>. Currently, <code>DoubleML</code> implements the double / debiased machine learning framework as established in <span class="citation">Chernozhukov et al. (2018)</span> for</p>
<ul>
<li>partially linear regression models (PLR),</li>
<li>partially linear instrumental variable regression models (PLIV),</li>
<li>interactive regression models (IRM), and</li>
<li>interactive instrumental variable regression models (IIVM).</li>
</ul>
<p>The object-oriented implementation of <code>DoubleML</code> is very flexible. The model classes <code>DoubleMLPLR</code>, <code>DoubleMLPLIV</code>, <code>DoubleMLIRM</code> and <code>DoubleIIVM</code> implement the estimation of the nuisance functions via machine learning methods and the computation of the Neyman-orthogonal score function. All other functionalities are implemented in the abstract base class <code>DoubleML</code>, including estimation of causal parameters, standard errors, <span class="math inline">\(t\)</span>-tests, confidence intervals, as well as valid simultaneous inference through adjustments of <span class="math inline">\(p\)</span>-values and estimation of joint confidence regions based on a multiplier bootstrap procedure. In combination with the estimation and tuning functionalities of <code>mlr3</code> and its ecosystem, this object-oriented implementation enables a high flexibility for the model specification in terms of</p>
<ul>
<li>the machine learning methods for estimation of the nuisance functions,</li>
<li>the resampling schemes,</li>
<li>the double machine learning algorithm, and</li>
<li>the Neyman-orthogonal score functions.</li>
</ul>
<p>It further can be readily extended regarding</p>
<ul>
<li>new model classes that come with Neyman-orthogonal score functions being linear in the target parameter,</li>
<li>alternative score functions via callables, and</li>
<li>customized resampling schemes.</li>
</ul>
<p>Several other packages for estimation of causal effects based on machine learning methods exist for R. Probably the most popular packages are the <code>grf</code> package <span class="citation">(Tibshirani, Athey, and Wager 2020)</span>, which implements generalized random forests <span class="citation">(Athey, Tibshirani, and Wager 2019)</span>, the package <code>hdm</code> <span class="citation">(Chernozhukov, Hansen, and Spindler 2016)</span> for inference based on the lasso estimator and the <code>hdi</code> package <span class="citation">(Dezeure et al. 2015)</span> for inference in high-dimensional models. Previous implementations of the double machine learning (DML) framework of <span class="citation">Chernozhukov et al. (2018)</span> have been provided by <code>postDoubleR</code> package <span class="citation">(Szitas 2019)</span>, the package <code>dmlmt</code> <span class="citation">(Knaus 2021)</span> with a focus on lasso estimation, and <code>causalDML</code> <span class="citation">(Knaus 2020)</span> for estimation of treatment effects under unconfoundedness. A variety of causal estimation methods, including treatment effect estimators that are based on double machine learning and causal mediation analysis, is implemented in <code>causalweight</code> <span class="citation">(Bodory and Huber 2020)</span>. The R package <code>AIPW</code> <span class="citation">(Zhong and Naimi 2021)</span> implements estimation of average treatment effects by augmented inverse probability weighting based on machine learning algorithms.</p>
<p>In python, <code>EconML</code> <span class="citation">(Microsoft Research 2019)</span> offers an implementation of the double machine learning framework for heterogeneous effects. We would like to mention that the R package <code>DoubleML</code> was developed together with a Python twin <span class="citation">(Bach et al. 2021)</span> that is based on <code>scikit-learn</code> <span class="citation">(Pedregosa et al. 2011)</span>. The python package is also available via <a href="https://github.com/DoubleML/doubleml-for-py">GitHub</a>, the <a href="https://pypi.org/project/DoubleML/0.1.2/">Python Package Index (PyPI)</a>, and <a href="https://anaconda.org/conda-forge/doubleml">conda-forge</a>. Moreover, <span class="citation">Kurz (2021)</span> provides a serverless implementation of the python module <code>DoubleML</code>.</p>
<p>The rest of the paper is structured as follows: In Section , we briefly demonstrate how to install the <code>DoubleML</code> package and give a short motivating example to illustrate the major idea behind the double machine learning approach. Section  introduces the main causal model classes implemented in <code>DoubleML</code>. Section  shortly summarizes the main ideas behind the double machine learning approach and reviews the key ingredients required for valid inference based on machine learning methods. Section  presents the main steps and algorithms of the double machine learning procedure for inference on one or multiple target parameters. Section  provides more detailed insights on the implemented classes and methods of <code>DoubleML</code>. Section  contains real-data and simulation examples for estimation of causal parameters using the <code>DoubleML</code> package. Additionally, this section provides a brief simulation study that illustrates the validity of the implemented methods in finite samples. Section  concludes the paper. The code output that has been suppressed in the main text and further information regarding the simulations are presented in the Appendix. To make the code examples fully reproducible, the entire code is available online.</p>
<p>The latest CRAN release of <code>DoubleML</code> can be installed using the command</p>
<p></p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">install.packages</span>(<span class="st">&quot;DoubleML&quot;</span>)</span></code></pre></div>
<p></p>
<p>Alternatively, the development version can be downloaded and installed from the GitHub<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> repository using the command (previous installation of the <code>remotes</code> package is required)</p>
<p></p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>remotes<span class="sc">::</span><span class="fu">install_github</span>(<span class="st">&quot;DoubleML/doubleml-for-r&quot;</span>)</span></code></pre></div>
<p></p>
<p>Among others, <code>DoubleML</code> depends on the R package <code>R6</code> for object oriented implementation, <code>data.table</code> <span class="citation">(Dowle and Srinivasan 2020)</span> for the underlying data structure, as well as the packages <code>mlr3</code> <span class="citation">(Lang et al. 2019)</span>, <code>mlr3learners</code> <span class="citation">(Lang, Au, et al. 2020)</span> and <code>mlr3tuning</code> <span class="citation">(Becker et al. 2020)</span> for estimation of machine learning methods, model tuning and parameter handling. Moreover, the underlying packages of the machine learning methods that are called in <code>mlr3</code> or <code>mlr3learners</code> must be installed, for example the packages <code>glmnet</code> for lasso estimation <span class="citation">(Friedman, Hastie, and Tibshirani 2010)</span> or <code>ranger</code> <span class="citation">(M. N. Wright and Ziegler 2017)</span> for random forests.</p>
<p>Load the package after completed installation.</p>
<p></p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(DoubleML)</span></code></pre></div>
<p></p>
<p>In the following, we provide a brief summary of and motivation to double machine learning methods and show how the corresponding methods provided by the <code>DoubleML</code> package can be applied. The data generating process (DGP) is based on the introductory example in <span class="citation">Chernozhukov et al. (2018)</span>. We consider a partially linear model: Our major interest is to estimate the causal parameter <span class="math inline">\(\theta\)</span> in the following regression equation <span class="math display">\[\begin{align*}
\begin{aligned}
y_i = \theta d_i + g_0(x_i) + \zeta_i, &amp; &amp;\zeta_i \sim \mathcal{N}(0,1),
\end{aligned}
\end{align*}\]</span> with covariates <span class="math inline">\(x_i \sim \mathcal{N}(0, \Sigma)\)</span>, where <span class="math inline">\(\Sigma\)</span> is a matrix with entries <span class="math inline">\(\Sigma_{kj} = 0.7^{|j-k|}\)</span>. In the following, the regression relationship between the treatment variable <span class="math inline">\(d_i\)</span> and the covariates <span class="math inline">\(x_i\)</span> will play an important role <span class="math display">\[\begin{align*}
\begin{aligned}
d_i = m_0(x_i) + v_i, &amp; &amp;v_i \sim \mathcal{N}(0,1).\\
\end{aligned}
\end{align*}\]</span> The nuisance functions <span class="math inline">\(m_0\)</span> and <span class="math inline">\(g_0\)</span> are given by <span class="math display">\[\begin{align*}
m_0(x_i) &amp;=  x_{i,1} + \frac{1}{4} \frac{\exp(x_{i,3})}{1+\exp(x_{i,3})}, \\
g_0(x_i) &amp;= \frac{\exp(x_{i,1})}{1+\exp(x_{i,1})} + \frac{1}{4} x_{i,3}.
\end{align*}\]</span> We construct a setting with <span class="math inline">\(n=500\)</span> observations and <span class="math inline">\(p=20\)</span> explanatory variables to demonstrate the use of the estimators provided in <code>DoubleML</code>. Moreover, we set the true value of the parameter <span class="math inline">\(\theta\)</span> to <span class="math inline">\(\theta=0.5\)</span>. The corresponding data generating process is implemented in the function <code>make_plr_CCDHNR2018()</code>. We start by generating a realization of a data set as a <code>data.table</code> object, which is subsequently used to create an instance of the data-backend of class <code>DoubleMLData</code>.</p>
<p></p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(DoubleML)</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>alpha <span class="ot">=</span> <span class="fl">0.5</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>n_obs <span class="ot">=</span> <span class="dv">500</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>n_vars <span class="ot">=</span> <span class="dv">20</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1234</span>)</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>data_plr <span class="ot">=</span> <span class="fu">make_plr_CCDDHNR2018</span>(<span class="at">alpha =</span> alpha, <span class="at">n_obs =</span> n_obs, <span class="at">dim_x =</span> n_vars,</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>                                <span class="at">return_type =</span> <span class="st">&quot;data.table&quot;</span>)</span></code></pre></div>
<p> The data-backend implements the causal model: We specify that we perform inference on the effect of the treatment variable <span class="math inline">\(d_i\)</span> on the dependent variable <span class="math inline">\(y_i\)</span>.</p>
<p></p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>obj_dml_data <span class="ot">=</span> DoubleMLData<span class="sc">$</span><span class="fu">new</span>(data_plr, <span class="at">y_col =</span> <span class="st">&quot;y&quot;</span>, <span class="at">d_cols =</span> <span class="st">&quot;d&quot;</span>)</span></code></pre></div>
<p> In the next step, we choose the machine learning method as an object of class <code>Learner</code> from <code>mlr3</code>, <code>mlr3learners</code> <span class="citation">(Lang, Au, et al. 2020)</span> or <code>mlr3extralearners</code> <span class="citation">(Sonabend and Schratz 2020)</span>. As we will point out later, we have to estimate two nuisance parts in order to perform valid inference in the partially linear regression model. Hence, we have to specify two learners. Moreover, we split the sample into two folds used for cross-fitting.</p>
<p></p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load mlr3 and mlr3learners package and suppress output during estimation</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(mlr3)</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(mlr3learners)</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>lgr<span class="sc">::</span><span class="fu">get_logger</span>(<span class="st">&quot;mlr3&quot;</span>)<span class="sc">$</span><span class="fu">set_threshold</span>(<span class="st">&quot;warn&quot;</span>) </span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize a random forests learner with specified parameters</span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>ml_g <span class="ot">=</span> <span class="fu">lrn</span>(<span class="st">&quot;regr.ranger&quot;</span>, <span class="at">num.trees =</span> <span class="dv">100</span>, <span class="at">mtry =</span> n_vars, <span class="at">min.node.size =</span> <span class="dv">2</span>, </span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>           <span class="at">max.depth =</span> <span class="dv">5</span>)</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>ml_m <span class="ot">=</span> <span class="fu">lrn</span>(<span class="st">&quot;regr.ranger&quot;</span>, <span class="at">num.trees =</span> <span class="dv">100</span>, <span class="at">mtry =</span> n_vars, <span class="at">min.node.size =</span> <span class="dv">2</span>, </span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>           <span class="at">max.depth =</span> <span class="dv">5</span>)</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>doubleml_plr <span class="ot">=</span> DoubleMLPLR<span class="sc">$</span><span class="fu">new</span>(obj_dml_data,</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>                               ml_g, ml_m,</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>                               <span class="at">n_folds =</span> <span class="dv">2</span>,</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>                               <span class="at">score =</span> <span class="st">&quot;IV-type&quot;</span>)</span></code></pre></div>
<p> To estimate the causal effect of variable <span class="math inline">\(d_i\)</span> on <span class="math inline">\(y_i\)</span>, we call the <code>fit()</code> method.</p>
<p></p>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>doubleml_plr<span class="sc">$</span><span class="fu">fit</span>()</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>doubleml_plr<span class="sc">$</span><span class="fu">summary</span>()</span></code></pre></div>
<pre><code>## [1] &quot;Estimates and significance testing of the effect of target variables&quot;
##   Estimate. Std. Error t value Pr(&gt;|t|)    
## d   0.49398    0.04852   10.18   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p> The output shows that the estimated coefficient is close to the true parameter <span class="math inline">\(\theta=0.5\)</span>. Moreover, we are able to reject the null hypotheses <span class="math inline">\(H_0: \theta=0\)</span> at all common significance levels.</p>
<!-- TODO: Introduce/refer to examples... -->
<p><code>DoubleML</code> provides estimation of causal effects in four different models: Partially linear regression models (PLR), partially linear instrumental variable regression models (PLIV), interactive regression models (IRM) and interactive instrumental variable regression models (IIVM). We will shortly introduce these models.</p>
<p>Partially linear regression models (PLR), which encompass the standard linear regression model, play an important role in data analysis <span class="citation">(Robinson 1988)</span>. Partially linear regression models take the form <span class="math display">\[\begin{align}
Y = D \theta_0 + g_0(X) + \zeta, \quad &amp;\mathbb{E}(\zeta | D,X) = 0, \label{plr1}\\
D = m_0(X) + V, \quad &amp;\mathbb{E}(V | X) = 0,   \label{plr2}
\end{align}\]</span> where <span class="math inline">\(Y\)</span> is the outcome variable and <span class="math inline">\(D\)</span> is the policy variable of interest. The high-dimensional vector <span class="math inline">\(X=(X_1,…,X_p)\)</span> consists of other confounding covariates, and <span class="math inline">\(\zeta\)</span> and <span class="math inline">\(V\)</span> are stochastic errors. Equation () is the equation of interest, and <span class="math inline">\(\theta_0\)</span> is the main regression coefficient that we would like to infer. If <span class="math inline">\(D\)</span> is conditionally exogenous (randomly assigned conditional on X), <span class="math inline">\(\theta_0\)</span> has the interpretation of a structural or causal parameter. The causal diagram supporting such interpretation is shown in Figure . The second equation keeps track of confounding, namely the dependence of <span class="math inline">\(D\)</span> on covariates/controls. The characteristics <span class="math inline">\(X\)</span> affect the policy variable <span class="math inline">\(D\)</span> via the function <span class="math inline">\(m_0(X)\)</span> and the outcome variable via the function <span class="math inline">\(g_0(X)\)</span>. The partially linear model generalizes both linear regression models, where functions <span class="math inline">\(g_0\)</span> and <span class="math inline">\(m_0\)</span> are linear with respect to a dictionary of basis functions with respect to <span class="math inline">\(X\)</span>, and approximately linear models.</p>
<p>We next consider the partially linear instrumental variable regression model: <span class="math display">\[\begin{align}
Y - D \theta_0 =  g_0(X) + \zeta, \quad &amp;\mathbb{E}(\zeta | Z, X) = 0, \label{pliv1} \\
Z = m_0(X) + V,  \quad &amp; \mathbb{E}(V | X) = 0. \label{pliv2}
\end{align}\]</span> Note that this model is not a regression model unless <span class="math inline">\(Z=D\)</span>. Model ()-() is a canonical model in causal inference, going back to <span class="citation">P. G. Wright (1928)</span>, with the modern difference being that <span class="math inline">\(g_0\)</span> and <span class="math inline">\(m_0\)</span> are nonlinear, potentially complicated functions of high-dimensional <span class="math inline">\(X\)</span>. The idea of this model is that there is a structural or causal relation between <span class="math inline">\(Y\)</span> and <span class="math inline">\(D\)</span>, captured by <span class="math inline">\(\theta_0\)</span>, and <span class="math inline">\(g_0(X) + \zeta\)</span> is the stochastic error, partly explained by covariates <span class="math inline">\(X\)</span>. <span class="math inline">\(V\)</span> and <span class="math inline">\(\zeta\)</span> are stochastic errors that are not explained by <span class="math inline">\(X\)</span>. Since <span class="math inline">\(Y\)</span> and <span class="math inline">\(D\)</span> are jointly determined, we need an external factor, commonly referred to as an instrument, <span class="math inline">\(Z\)</span>, to create exogenous variation in <span class="math inline">\(D\)</span>. Note that <span class="math inline">\(Z\)</span> should affect <span class="math inline">\(D\)</span>. The <span class="math inline">\(X\)</span> here serve again as confounding factors, so we can think of variation in <span class="math inline">\(Z\)</span> as being exogenous only conditional on <span class="math inline">\(X\)</span>.</p>
<p>A simple contextual example is from biostatistics <span class="citation">(Permutt and Hebel 1989)</span>, where <span class="math inline">\(Y\)</span> is a health outcome and <span class="math inline">\(D\)</span> is an indicator of smoking. Thus, <span class="math inline">\(\theta_0\)</span> captures the effect of smoking on health. Health outcome <span class="math inline">\(Y\)</span> and smoking behavior <span class="math inline">\(D\)</span> are treated as being jointly determined. <span class="math inline">\(X\)</span> represents patient characteristics, and <span class="math inline">\(Z\)</span> could be a doctor’s advice not to smoke (or another behavioral treatment) that may affect the outcome <span class="math inline">\(Y\)</span> only through shifting the behavior <span class="math inline">\(D\)</span>, conditional on characteristics <span class="math inline">\(X\)</span>.</p>
<p>We consider estimation of average treatment effects when treatment effects are fully heterogeneous and the treatment variable is binary, <span class="math inline">\(D\in \{0, 1\}\)</span>. We consider vectors <span class="math inline">\((Y, D, X)\)</span> such that <span class="math display">\[\begin{align}
Y = g_0(D, X) + U, \quad &amp; \mathbb{E}(U | X, D) = 0,  \label{irm1}\\
D = m_0(X) + V, \quad &amp;\mathbb{E}(V | X) = 0. \label{irm2}
\end{align}\]</span> Since <span class="math inline">\(D\)</span> is not additively separable, this model is more general than the partially linear model for the case of binary <span class="math inline">\(D\)</span>. A common target parameter of interest in this model is the average treatment effect (ATE), <span class="math display">\[\begin{align*}
\theta_0 = \mathbb{E}[g_0(1, X) - g_0(0,X)].
\end{align*}\]</span> Another common target parameter is the average treatment effect for the treated (ATTE), <span class="math display">\[\begin{align*}
\theta_0 = \mathbb{E}[g_0(1, X) - g_0(0,X) | D=1].
\end{align*}\]</span> In business applications, the ATTE is often the main interest, as it captures the treatment effect for those who have been affected by the treatment. A difference of the ATTE from the ATE might arise if the characteristics of the treated individuals differ from those of the general population.</p>
<p>The confounding factors <span class="math inline">\(X\)</span> affect the policy variable via the propensity score <span class="math inline">\(m_0(X)\)</span> and the outcome variable via the function <span class="math inline">\(g_0(X)\)</span>. Both of these functions are unknown and potentially complex, and we can employ ML methods to learn them.</p>
<p>We consider estimation of local average treatment effects (LATE) with a binary treatment variable <span class="math inline">\(D\in\{0,1\}\)</span>, and a binary instrument, <span class="math inline">\(Z\in\{0,1\}\)</span>. As before, <span class="math inline">\(Y\)</span> denotes the outcome variable, and <span class="math inline">\(X\)</span> is the vector of covariates. Here the structural equation model is: <span class="math display">\[\begin{align}
Y = \ell_0(D, X) + \zeta,  \quad &amp; \mathbb{E}(\zeta | Z, X) = 0, \label{iivm1}\\
Z = m_0(X) + V, \quad &amp;\mathbb{E}(V | X) = 0.\label{iivm2}
\end{align}\]</span> Consider the functions <span class="math inline">\(g_0\)</span>, <span class="math inline">\(r_0\)</span>, and <span class="math inline">\(m_0\)</span>, where <span class="math inline">\(g_0\)</span> maps the support of <span class="math inline">\((Z,X)\)</span> to <span class="math inline">\(\mathbb{R}\)</span> and <span class="math inline">\(r_0\)</span> and <span class="math inline">\(m_0\)</span> map the support of <span class="math inline">\((Z,X)\)</span> and <span class="math inline">\(X\)</span> to <span class="math inline">\((\epsilon, 1-\epsilon)\)</span> for some <span class="math inline">\(\epsilon \in (0, 1/2)\)</span>, such that <span class="math display">\[\begin{align}
Y = g_0(Z, X) + \nu, \quad &amp;\mathbb{E}(\nu | Z, X) = 0,\\
D = r_0(Z, X) + U,  \quad &amp; \mathbb{E}(U | Z, X) = 0,\\
Z = m_0(X) + V, \quad &amp; \mathbb{E}(V | X) = 0.
\end{align}\]</span> We are interested in estimating <span class="math display">\[\begin{align*} 
\theta_0 &amp;= \frac{\mathbb{E}[g_0(1, X)] - \mathbb{E}[g_0(0,X)]}{\mathbb{E}[r_0(1, X)] - \mathbb{E}[r_0(0,X)]}.
\end{align*}\]</span> Under the well-known assumptions of <span class="citation">Imbens and Angrist (1994)</span>, <span class="math inline">\(\theta_0\)</span> is the LATE – the average treatment effect for compliers, in other words, those observations that would have <span class="math inline">\(D=1\)</span> if <span class="math inline">\(Z\)</span> were <span class="math inline">\(1\)</span> and would have <span class="math inline">\(D=0\)</span> if <span class="math inline">\(Z\)</span> were <span class="math inline">\(0\)</span>.</p>
<p>Here we provide an intuitive discussion of how double machine learning works in the first model, the partially linear regression model. Naive application of machine learning methods directly to equations ()-() may have a very high bias. Indeed, it can be shown that small biases in estimation of <span class="math inline">\(g_0\)</span>, which are unavoidable in high-dimensional estimation, create a bias in the naive estimate of the main effect, <span class="math inline">\(\hat{\theta_0}^{naive}\)</span>, which is sufficiently large to cause failure of conventional inference. The left panel in Figure  illustrates this phenomenon. The histogram presents the empirical distribution of the studentized estimator, <span class="math inline">\(\hat{\theta}^{naive}_0\)</span>, as obtained in <span class="math inline">\(1000\)</span> independent repetitions of the data generating process presented in Section . The functions <span class="math inline">\(g_0\)</span> and <span class="math inline">\(m_0\)</span> in the PLR model are estimated with random forest learners and corresponding predictions are then plugged into a non-orthogonal score function. The regularization performed by the random forest learner leads to a bias in estimation of <span class="math inline">\(g_0\)</span> and <span class="math inline">\(m_0\)</span>. Due to non-orthogonality of the score, this translates into a considerable bias of the main estimator <span class="math inline">\(\hat{\theta_0}^{naive}\)</span>: The distribution of the studentized estimator <span class="math inline">\(\hat{\theta}^{naive}_0\)</span> is shifted to the left of the origin and differs substantially from a normal distribution that would be obtained if the regularization bias was negligible as shown by the red curve.</p>
<p>The PLR model above can be rewritten in the following residualized form: <span class="math display">\[\begin{align}
W = V \theta_0 + \zeta,\quad &amp; \mathbb{E}(\zeta | D, X) = 0, \label{p_out1} \\
W = (Y - \ell_0(X)), \quad &amp; \ell_0(X) = \mathbb{E}[Y|X], \label{p_out2} \\
V = (D- m_0(X)), \quad &amp; m_0(X) = \mathbb{E}[D|X]. \label{p_our3}
\end{align}\]</span> The variables <span class="math inline">\(W\)</span> and <span class="math inline">\(V\)</span> represent original variables after taking out or <em>partialling out</em> the effect of <span class="math inline">\(X\)</span>. Note that <span class="math inline">\(\theta_0\)</span> is identified from this equation if <span class="math inline">\(V\)</span> has a non-zero variance.</p>
<p>Our goal is to construct high-quality point and interval estimators for <span class="math inline">\(\theta_0\)</span> when <span class="math inline">\(X\)</span> is high-dimensional and we employ machine learning methods to estimate the nuisance functions such as <span class="math inline">\(g_0\)</span> and <span class="math inline">\(m_0\)</span>. Example ML methods include lasso, random forests, boosted trees, deep neural networks, and ensembles or aggregated versions of these methods.</p>
We shall use a method-of-moments estimator for <span class="math inline">\(\theta_0\)</span> based upon the empirical analog of the moment condition <span class="math display">\[\begin{align}
\mathbb{E}[\psi(W; \theta_0, \eta_0)] = 0, \label{score}
\end{align}\]</span> where we call <span class="math inline">\(\psi\)</span> the score function, <span class="math inline">\(W = (Y,D,X,Z)\)</span>, <span class="math inline">\(\theta_0\)</span> is the parameter of interest, and <span class="math inline">\(\eta\)</span> denotes nuisance functions with population value <span class="math inline">\(\eta_0\)</span>.
<p>Neyman orthogonality () ensures that the moment condition () used to identify and estimate <span class="math inline">\(\theta_0\)</span> is insensitive to small pertubations of the nuisance function <span class="math inline">\(\eta\)</span> around <span class="math inline">\(\eta_0\)</span>. The derivative <span class="math inline">\(\partial_{\eta}\)</span> denotes the pathwise (Gateaux) derivative operator.</p>
<p>Using a Neyman-orthogonal score eliminates the first order biases arising from the replacement of <span class="math inline">\(\eta_0\)</span> with a ML estimator <span class="math inline">\(\hat{\eta}_0\)</span>. Eliminating this bias is important because estimators <span class="math inline">\(\hat{\eta}_0\)</span> must be heavily regularized in high dimensional settings to be good estimators of <span class="math inline">\(\eta_0\)</span>, and so these estimators will be biased in general. The Neyman orthogonality property is responsible for the adaptivity of these estimators – namely, their approximate distribution will not depend on the fact that the estimate <span class="math inline">\(\hat{\eta}_0\)</span> contains error, if the latter is mild.</p>
<p>The right panel of Figure  presents the empirical distribution of the studentized DML estimator <span class="math inline">\(\tilde{\theta}_0\)</span> that is based on an orthogonal score. Note that estimation is performed on the identical simulated data sets and with the same machine learning method as for the naive learner, which is displayed in the left panel. The histogram of the studentized estimator <span class="math inline">\(\tilde{\theta}_0\)</span> illustrates the favorable performance of the double machine learning estimator, which is based on an orthogonal score: The DML estimator is robust to the bias that is generated by regularization. The estimator is approximately unbiased, is concentrated around <span class="math inline">\(0\)</span> and the distribution is well-approximated by the normal distribution.</p>
<ul>
<li><p><strong>PLR score:</strong> In the PLR model, we can employ two alternative score functions. We will shortly indicate the option for initialization of a model object in <code>DoubleML</code> to clarify how each score can be implemented. Using the option <code>score = &quot;partialling out&quot;</code> leads to estimation of the score function <span class="math display">\[\begin{align}  \label{scorepartiallingout}
\begin{aligned}
&amp;\psi(W;\theta, \eta) := \left(Y - \ell(X) - \theta(D-m(X))\right) \left(D - m(X)\right),  \\
&amp; \eta = (\ell,m), \quad  \eta_0 = (\ell_0, m_0),
\end{aligned}
\end{align}\]</span> where <span class="math inline">\(W=(Y,D,X)\)</span> and <span class="math inline">\(\ell\)</span> and <span class="math inline">\(m\)</span> are <span class="math inline">\(P\)</span>-square-integrable functions mapping the support of <span class="math inline">\(X\)</span> to <span class="math inline">\(\mathbb{R}\)</span>, whose true values are given by <span class="math display">\[\begin{align*}
\ell_0(X) = \mathbb{E}[Y|X], \quad  m_0(X) = \mathbb{E}[D|X].
\end{align*}\]</span> Alternatively, it is possible to use the following score function for the PLR via the option <code>score = &quot;IV-type&quot;</code> <span class="math display">\[\begin{align}
\begin{aligned}
&amp; \psi(W;\theta, \eta) := \left(Y-D\theta - g(X)\right) \left(D-m(X)\right), 
&amp; \eta = (g, m), \quad  \eta_0 = (g_0, m_0),
\end{aligned}
\end{align}\]</span> with <span class="math inline">\(g\)</span> and <span class="math inline">\(m\)</span> being <span class="math inline">\(P\)</span>-square-integrable functions mapping the support of <span class="math inline">\(X\)</span> to <span class="math inline">\(\mathbb{R}\)</span> with values given by <span class="math display">\[\begin{align*}
g_0 = \mathbb{E}[Y|X], \quad m_0(X) = \mathbb{E}[D|X].
\end{align*}\]</span> The scores above are Neyman-orthogonal by elementary calculations. Now, it is possible to see the connections to the residualized system of equations presented in Section .</p></li>
<li><p><strong>PLIV score:</strong> In the PLIV model, we employ the score function (<code>score = &quot;partialling out&quot;</code>) <span class="math display">\[\begin{align}
\begin{aligned}
&amp;\psi(W;\theta, \eta) := \left(Y - \ell(x) - \theta(D-r(X))\right) \left(Z - m(X) \right), \\
&amp; \eta = (\ell, m, r), \quad \eta_0 = (\ell_0, m_0, r_0),
\end{aligned}
\end{align}\]</span> where <span class="math inline">\(W=(Y,D,X,Z)\)</span> and <span class="math inline">\(\ell\)</span>, <span class="math inline">\(m\)</span>, and <span class="math inline">\(r\)</span> are <span class="math inline">\(P\)</span>-square integrable functions mapping the support of <span class="math inline">\(X\)</span> to <span class="math inline">\(\mathbb{R}\)</span>, whose true values are given by <span class="math display">\[\begin{align*}
\ell_0(X) = \mathbb{E}[Y|X], \quad r_0(X) = \mathbb{E}[D|X], \quad m_0(X) = \mathbb{E}[Z|X].
\end{align*}\]</span></p></li>
<li><p><strong>IRM score:</strong> For estimation of the ATE parameter of the IRM model, we employ the score (<code>score = &quot;ATE&quot;</code>) <span class="math display">\[\begin{align}
\begin{aligned}
&amp; \psi(W;\theta, \eta) := \left(g(1,X) - g(0,X) \right) + \frac{D(Y-g(1,X))}{m(X)} - \frac{(1-D)(Y-g(0,X))}{1-m(X)} - \theta, \\
&amp; \eta = (g,m), \quad \eta_0 = (g_0, m_0),
\end{aligned}
\end{align}\]</span> where <span class="math inline">\(W=(Y,D,X)\)</span> and <span class="math inline">\(g\)</span> and <span class="math inline">\(m\)</span> map the support of <span class="math inline">\((D,X)\)</span> to <span class="math inline">\(\mathbb{R}\)</span> and the support of <span class="math inline">\(X\)</span> to <span class="math inline">\((\epsilon, 1-\epsilon)\)</span>, respectively, for some <span class="math inline">\(\epsilon \in (0, 1/2)\)</span>, whose true values are given by <span class="math display">\[\begin{align*}
g_0(D,X)=\mathbb{E}[Y|D,X], \quad m_0(x)=\mathbb{P}[D=1|X].
\end{align*}\]</span> This orthogonal score is based on the influence function for the mean for missing data from <span class="citation">Robins and Rotnitzky (1995)</span>. For estimation of the ATTE parameter in the IRM, we use the score (<code>score = &quot;ATTE&quot;</code>) <span class="math display">\[\begin{align}
\begin{aligned}
&amp;\psi(W;\theta, \eta) := \frac{D(Y-g(0,X))}{p} - \frac{m(X)(1-D)(Y-g(0,X))}{p(1-m(x))} - \frac{D}{p}\theta,\\
&amp; \eta = (g, m, p), \quad \eta_0 = (g_0, m_0, p_0),
\end{aligned}
\end{align}\]</span> where <span class="math inline">\(p_0=\mathbb{P}(D=1)\)</span>. Note that this score does not require estimating <span class="math inline">\(g_0(1,X)\)</span>.</p></li>
<li><p><strong>IIVM score:</strong> To estimate the LATE paramter in the IIVM, we will use the score (<code>score = &quot;LATE&quot;</code>) <span class="math display">\[\begin{align}
\begin{aligned}
\psi := &amp; g(1,X) - g(0,X) + \frac{Z(Y-g(1,X))}{m(X)} - \frac{(1-Z)(Y-g(0,X))}{1-m(X)} \\
&amp; - \left( r(1,x) - r(0,X) + \frac{Z(D-r(1,x)}{m(X)} - \frac{(1-Z)(D-r(0,X)}{1-m(X)} \right) \times \theta,\\
&amp; \eta = (g, m, r), \quad \eta_0 = (g_0, m_0, r_0),
\end{aligned}
\end{align}\]</span> where <span class="math inline">\(W=(Y,D,X,Z)\)</span> and the nuisance parameter <span class="math inline">\(\eta=(g, m, r)\)</span> consists of <span class="math inline">\(P\)</span>-square integrable functions <span class="math inline">\(g\)</span>, <span class="math inline">\(m\)</span>, and <span class="math inline">\(r\)</span>, with <span class="math inline">\(g\)</span> mapping the support of <span class="math inline">\((Z,X)\)</span> to <span class="math inline">\(\mathbb{R}\)</span> and <span class="math inline">\(m\)</span> and <span class="math inline">\(r\)</span>, respectively, mapping the support of <span class="math inline">\((Z,X)\)</span> and <span class="math inline">\(X\)</span> to <span class="math inline">\((\epsilon, 1-\epsilon)\)</span> for some <span class="math inline">\(\epsilon \in (0,1/2)\)</span>.</p></li>
</ul>
<p>For instance, in the PLR model, we need to have access to consistent estimators of <span class="math inline">\(g_0\)</span> and <span class="math inline">\(m_0\)</span> with respect to the <span class="math inline">\(L^2(P)\)</span> norm <span class="math inline">\(\lVert \cdot \lVert_{P,2}\)</span>, such that <span class="math display">\[\begin{align}
\lVert \hat{m}_0 - m_0 \lVert_{P,2} + \lVert \hat{\ell}_0 - \ell_0 \lVert_{P,2} \le o(N^{-1/4}).
\end{align}\]</span> In the PLIV model, the sufficient condition is <span class="math display">\[\begin{align}
\lVert \hat{r}_0 - r_0 \lVert_{P,2} + \lVert \hat{m}_0 - m_0 \lVert_{P,2} + \lVert \hat{\ell}_0 - \ell_0 \lVert_{P,2} \le o(N^{-1/4}).
\end{align}\]</span> These conditions are plausible for many ML methods. Different structured assumptions on <span class="math inline">\(\eta_0\)</span> lead to the use of different machine-learning tools for estimating <span class="math inline">\(\eta_0\)</span> as listed in <span class="citation">Chernozhukov et al. (2018, 22–23)</span>:</p>
<ol style="list-style-type: decimal">
<li>The assumption of approximate or exact sparsity for <span class="math inline">\(\eta_0\)</span> with respect to some dictionary calls for the use of sparsity-based machine learning methods, for example the lasso estimator, post-lasso, <span class="math inline">\(l_2\)</span>-boosting, or forward selection, among others.</li>
<li>The assumption of density of <span class="math inline">\(\eta_0\)</span> with respect to some dictionary calls for density-based estimators such as the ridge. Mixed structures based on sparsity and density suggest the use of elastic net or lava.</li>
<li>If <span class="math inline">\(\eta_0\)</span> can be well approximated by tree-based methods, regression trees and random forests are suitable.</li>
<li>If <span class="math inline">\(\eta_0\)</span> can be well approximated by sparse, shallow or deep neural networks, <span class="math inline">\(l_1\)</span>-penalized neural networks, shallow neural networks or deep neural networks are attractive.</li>
</ol>
<p>For most of these ML methods, performance guarantees are available that make it possible to satisfy the theoretical requirements. Moreover, if <span class="math inline">\(\eta_0\)</span> can be well approximated by at least one model mentioned in the list above, ensemble or aggregated methods can be used. Ensemble and aggregation methods ensure that the performance guarantee is approximately no worse than the performance of the best method.</p>
<!-- 1. approximate sparsity for $\eta_0$ with respect to some dictionary calls for the use of forward selection, lasso, post-lasso, $l_2$-boosting, or some other sparsity-based technique,  -->
<!-- 1. well-approximability of $\eta_0$ by trees calls for the use of regression trees and random forests,  -->
<!-- 1. well-approximability of $\eta_0$ by sparse neural and deep neural networks calls for the use of $l_1$-penalized neural and deep neural networks,  -->
<!-- 1. well-approximability of $\eta_0$ by at least one model mentioned in 1.-3. above calls for the use of an ensemble/aggregated method over the estimation methods mentioned in 1.-3.  -->
<!-- For most of these ML methods, performance guarantees are available that make it possible to satisfy the theoretical requirements.  -->
<!-- There are performance guarantees for most of these ML methods that make it possible to satisfy the conditions stated below. Ensemble and aggregation methods ensure that the performance guarantee is approximately no worse than the performance of the best method. -->
<p>Biases arising from overfitting could result from using highly complex fitting methods such as boosting, random forests, ensemble, and hybrid machine learning methods. We specifically use cross-fitted forms of the empirical moments, as detailed below in Algorithms 1 and 2, in estimation of <span class="math inline">\(\theta_0\)</span>. If we do not perform sample splitting and the ML estimates overfit, we may end up with very large biases. This is illustrated in Figure . The left panel shows the histogram of a studentized estimator <span class="math inline">\(\hat{\theta}^{nosplit}_0\)</span> with <span class="math inline">\(\hat{\theta}^{nosplit}_0\)</span> being obtained from solving the orthogonal score of Equation () without sample splitting. All observations are used to learn functions <span class="math inline">\(g_0\)</span> and <span class="math inline">\(m_0\)</span> in the PLR model and to solve the score <span class="math inline">\(\frac{1}{N}\sum_i^{N} \psi(W_i; \hat\theta^{nosplit}_0, \hat{\eta}_0)\)</span>. Consequently, this overfitting bias leads to a considerable shift of the empirical distribution to the left. The double machine learning estimator underlying the histogram in the right panel is obtained with cross-fitting according to Algorithm 2. The sample-splitting procedure makes it possible to completely eliminate the bias induced by overfitting.</p>
<p>We assume that we have a sample <span class="math inline">\((W_i)^N_{i_1}\)</span>, modeled as i.i.d. copies of <span class="math inline">\(W=(Y,D,Z,X)\)</span>, whose law is determined by the probability measure <span class="math inline">\(P\)</span>. We assume that <span class="math inline">\(N\)</span> is divisible by <span class="math inline">\(K\)</span> in order to simplify the notation. Let <span class="math inline">\(\mathbb{E}_N\)</span> denote the empirical expectation <span class="math display">\[\begin{align*}
\mathbb{E}_N[g(W)] := \frac{1}{N} \sum_{i=1}^{N}g(W_i).
\end{align*}\]</span></p>
<blockquote>
<p><strong>Remark 1</strong> (<em>Linear scores</em>) The score for the models PLR, PLIV, IRM and IIVM are linear in <span class="math inline">\(\theta\)</span>, having the form <span class="math display">\[\begin{align*}
\psi(W;\theta, \eta) = \psi_a(W; \eta) \theta + \psi_b(W; \eta),
\end{align*}\]</span> hence the estimator <span class="math inline">\(\tilde{\theta}_{0,k}\)</span> for DML2 (<span class="math inline">\(\check{\theta}_{0,k}\)</span> for DML1) takes the form <span class="math display">\[\begin{align} 
\tilde{\theta}_0 = - \left(\mathbb{E}_N[\psi_a(W; \eta)]\right)^{-1}\mathbb{E}_N[\psi_b(W; \eta)].\label{linearscore}
\end{align}\]</span></p>
</blockquote>
<p>The linear score function representations of the PLR, PLIV, IRM and IIVM are</p>
<ul>
<li><p><strong>PLR</strong> with <code>score = &quot;partialling out&quot;</code> <span class="math display">\[\begin{align}\begin{aligned}\psi_a(W; \eta) &amp;=  - (D - m(X)) (D - m(X)),\\\psi_b(W; \eta) &amp;= (Y - \ell(X)) (D - m(X)).\end{aligned}\end{align}\]</span> <strong>PLR</strong> with <code>score = &quot;IV-type&quot;</code> <span class="math display">\[\begin{align}\begin{aligned}\psi_a(W; \eta) &amp;=  - D (D - m(X)),\\\psi_b(W; \eta) &amp;= (Y - g(X)) (D - m(X)).\end{aligned}\end{align}\]</span></p></li>
<li><p><strong>PLIV</strong> with <code>score = &quot;partialling out&quot;</code> <span class="math display">\[\begin{align}\begin{aligned}\psi_a(W; \eta) &amp;=  - (D - r(X)) (Z - m(X)),\\\psi_b(W; \eta) &amp;= (Y - \ell(X)) (Z - m(X)).\end{aligned}\end{align}\]</span></p></li>
<li><p><strong>IRM</strong> with <code>score = &quot;ATE&quot;</code> <span class="math display">\[\begin{align}\begin{aligned}\psi_a(W; \eta) &amp;=  - 1,\\
\psi_b(W; \eta) &amp;= g(1,X) - g(0,X) + \frac{D (Y - g(1,X))}{m(X)} - \frac{(1 - D)(Y - g(0,X))}{1 - m(x)}.\end{aligned}\end{align}\]</span> <strong>IRM</strong> with <code>score = &quot;ATTE&quot;</code> <span class="math display">\[\begin{align}\begin{aligned}\psi_a(W; \theta, \eta)&amp;= -\frac{D}{p} \\
\psi_b(W; \theta, \eta) &amp;= \frac{D (Y - g(0,X))}{p} - \frac{m(X) (1 - D) (Y - g(0,X))}{p(1 - m(x))} \\ \end{aligned}\end{align}\]</span></p></li>
<li><p><strong>IIVM</strong> with <code>score = &quot;LATE&quot;</code> <span class="math display">\[\begin{align}\begin{aligned}\psi_a(W; \eta) &amp;=  - \bigg(r(1,X) - r(0,X) + \frac{Z (D - r(1,X))}{m(X)} - \frac{(1 - Z)(D - r(0,X))}{1 - m(x)} \bigg),\\\psi_b(W; \eta) &amp;= g(1,X) - g(0,X) + \frac{Z (Y - g(1,X))}{m(X)} - \frac{(1 - Z)(Y - g(0,X))}{1 - m(x)}.\end{aligned}\end{align}\]</span></p></li>
</ul>
<blockquote>
<p><strong>Remark 2</strong> (<em>Sample Splitting</em>) In Step (2) of the Algorithm DML1 and DML2, the estimator <span class="math inline">\(\hat{\eta}_{0,k}\)</span> can generally be an ensemble or aggregation of several estimators as long as we only use the data <span class="math inline">\((W_i)_{i\not\in I_k}\)</span> outside the <span class="math inline">\(k\)</span>-th fold to construct the estimators.</p>
</blockquote>
<blockquote>
<p><strong>Remark 3</strong> (<em>Recommendation</em>) We have found that <span class="math inline">\(K=4\)</span> or <span class="math inline">\(K=5\)</span> to work better than <span class="math inline">\(K=2\)</span> in a variety of empirical examples and in simulations. The default for the option <code>n_folds</code> that implements the value of <span class="math inline">\(K\)</span> is <code>n_folds=5</code>. Moreover, we generally recommend to repeat the estimation procedure mutliple times and use the estimates and standard errors as aggregated over multiple repetitions as described in <span class="citation">Chernozhukov et al. (2018, 30–31)</span>. This aggregation will be automatically executed if the number of repetitions <code>n_rep</code> is set to a value larger than 1.</p>
</blockquote>
<p>The properties of the estimator are as follows.</p>
<blockquote>
<p><strong>Remark 4</strong> (<em>Brief literature overview on double machine learning</em>) The presented double machine learning method was developed in <span class="citation">Chernozhukov et al. (2018)</span>. The idea of using property (16) to construct estimators and inference procedures that are robust to small mistakes in nuisance parameters can be traced back to <span class="citation">Neyman (1959)</span> and has been used explicitly or implicitly in the literature on debiased sparsity-based inference <span class="citation">(Belloni, Chernozhukov, and Hansen 2011; Belloni, Chernozhukov, and Wang 2014; Javanmard and Montanari 2014; van de Geer et al. 2014; Zhang and Zhang 2014; Chernozhukov, Hansen, and Spindler 2015b)</span> as well as (implicitly) in the classical semi-parametric learning theory with low-dimensional <span class="math inline">\(X\)</span> <span class="citation">(Bickel et al. 1993; Newey 1994; Van der Vaart 2000; Van der Laan and Rose 2011)</span>. These references also explain that if we use scores <span class="math inline">\(\psi\)</span> that are not Neyman-orthogonal in high dimensional settings, then the resulting estimators of <span class="math inline">\(\theta_0\)</span> are not <span class="math inline">\(1/\sqrt{N}\)</span> consistent and are generally heavily biased.</p>
</blockquote>
<blockquote>
<p><strong>Remark 5</strong> (<em>Literature on sample splitting</em>). Sample splitting has been used in the traditional semiparametric estimation literature to establish good properties of semiparametric estimators under weak conditions <span class="citation">(Schick 1986; Van der Vaart 2000)</span>. In sparse learning problems with high-dimensional <span class="math inline">\(X\)</span>, sample splitting was employed in <span class="citation">Belloni et al. (2012)</span>. There and here, the use of sample splitting results in weak conditions on the estimators of nuisance parameters, translating into weak assumptions on sparsity in the case of sparsity-based learning.</p>
</blockquote>
<blockquote>
<p><strong>Remark 6</strong> (<em>Debiased machine learning</em>). The presented approach builds upon and generalizes the approach of <span class="citation">Belloni, Chernozhukov, and Hansen (2011)</span>, <span class="citation">Zhang and Zhang (2014)</span>, <span class="citation">Javanmard and Montanari (2014)</span>, <span class="citation">Javanmard and Montanari (2014)</span>, <span class="citation">Javanmard and Montanari (2018)</span>, <span class="citation">Belloni, Chernozukov, and Hansen (2014)</span>, <span class="citation">Belloni, Chernozhukov, and Kato (2014)</span>, <span class="citation">Bühlmann and van de Geer (2015)</span>, which considered estimation of the special case ()-() using lasso without cross-fitting. This generalization, by relying upon cross-fitting, opens up the use of a much broader collection of machine learning methods and, in the case the lasso is used to estimate the nuisance functions, allows relaxation of sparsity conditions. All of these approaches can be seen as “debiasing” the estimation of the main parameter by constructing, implicitly or explicitly, score functions that satisfy the exact or approximate Neyman orthogonality.</p>
</blockquote>
<p>In addition to estimation of target causal parameters, standard errors, and confidence intervals, the package <code>DoubleML</code> provides methods to perform valid simultaneous inference based on a multiplier bootstrap procedure introduced in <span class="citation">Chernozhukov, Chetverikov, and Kato (2013)</span> and <span class="citation">Chernozhukov, Chetverikov, and Kato (2014)</span> and suggested in high-dimensional linear regression models in <span class="citation">Belloni, Chernozhukov, and Kato (2014)</span>. Accordingly, it is possible to (i) construct simultaneous confidence bands for a potentially large number of causal parameters and (ii) adjust <span class="math inline">\(p\)</span>-values in a test of multiple hypotheses based on the inferential procedure introduced above.</p>
<p>We consider a causal PLR with <span class="math inline">\(p_1\)</span> causal parameters of interest <span class="math inline">\(\theta_{0,1}, \ldots, \theta_{0,p_1}\)</span> associated with the treatment variables <span class="math inline">\(D_1, \ldots, D_{p_1}\)</span>. The parameter of interest <span class="math inline">\(\theta_{0,j}\)</span> with <span class="math inline">\(j=1, \ldots, p_1\)</span> solves a corresponding moment condition <span class="math display">\[\begin{align} \label{multiplecoefs}
\mathbb{E}\left[\psi_j(W;\theta_{0,j}, \eta_{0,j}) \right]=0,
\end{align}\]</span> as for example considered in <span class="citation">Belloni et al. (2018)</span>. To perform inference in a setting with multiple target coefficients <span class="math inline">\(\theta_{0,j}\)</span>, the double machine learning procedure implemented in <code>DoubleML</code> iterates over the target variables of interest. During estimation of the coefficient <span class="math inline">\(\theta_{0,j}\)</span>, i.e., estimating the effect of treatment <span class="math inline">\(D_j\)</span> on <span class="math inline">\(Y\)</span>, the remaining treatment variables enter the nuisance terms by default.</p>
<blockquote>
<p><strong>Remark 7</strong> (<em>Computational efficiency</em>) The multiplier bootstrap procedure of <span class="citation">Chernozhukov, Chetverikov, and Kato (2013)</span> and <span class="citation">Chernozhukov, Chetverikov, and Kato (2014)</span> is computatioanally efficient because it does not require resampling and reestimation of the causal parameters. Instead, it is sufficient to introduce a random pertubation of the score <span class="math inline">\(\psi\)</span> and solve for <span class="math inline">\(\theta_0\)</span>, accordingly.</p>
</blockquote>
<p>To construct simultaneous <span class="math inline">\((1-\alpha)\)</span>-confidence bands, the multiplier bootstrap presented in Algorithm 4 can be used to obtain a constant <span class="math inline">\(c_{1-\alpha}\)</span> that will guarantee asymptotic <span class="math inline">\((1-\alpha\)</span>) coverage <span class="math display">\[\begin{align} \label{confband}
\left[\tilde\theta_{0,j} \pm c_{1-\alpha} \cdot \hat\sigma_j/\sqrt{N} \right].
\end{align}\]</span> The constant <span class="math inline">\(c_{1-\alpha}\)</span> is obtained in two steps.</p>
<ol style="list-style-type: decimal">
<li>Calculate the maximum of the absolute values of the bootstrapped <span class="math inline">\(t\)</span>-statistics, <span class="math inline">\(t^{*,b}_j\)</span> in every repetition <span class="math inline">\(b\)</span> with <span class="math inline">\(b=1,\ldots,B\)</span>.</li>
<li>Use the <span class="math inline">\((1-\alpha)\)</span>-quantile of the <span class="math inline">\(B\)</span> maxima statistics from Step 1 as <span class="math inline">\(c_{1-\alpha}\)</span> and construct simultaneous confidence bands according to Equation ().</li>
</ol>
<p>Moreover, it is possible to derive an adjustment method for <span class="math inline">\(p\)</span>-values obtained from a test of multiple hypotheses, including classical adjustments such as the Bonferroni correction as well as the Romano-Wolf stepdown procedure <span class="citation">(Romano and Wolf 2005a, 2005b)</span>. The latter is implemented according to the algorithm for adjustment of <span class="math inline">\(p\)</span>-values as provided in <span class="citation">Romano and Wolf (2016)</span> and adapted to high-dimensional linear regression based on the lasso in <span class="citation">Bach, Chernozhukov, and Spindler (2018)</span>.</p>
<p>In this section, we briefly provide information on the implementation details such as the class structure, the data-backend and the use of machine learning methods. Section  provides a demonstration of <code>DoubleML</code> in real-data and simulation examples. More information on the implementation can be found in the DoubleML User Guide, that is available online<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a>. All class methods are documented in the documentation of the corresponding class, which can be browsed online<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a> or, for example, by using the commands <code>help(DoubleML)</code>, <code>help(DoubleMLPLR)</code>, or <code>help(DoubleMLData)</code> in R.</p>
<p>The implementation of <code>DoubleML</code> for R is based on object orientation as enabled by the the <code>R6</code> package <span class="citation">(Chang 2020)</span>. For an introduction to object orientation in R and the <code>R6</code> package, we refer to the vignettes of the <code>R6</code> package that are available online<a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a>, Chapter 2.1 of <span class="citation">Becker et al. (2021)</span>, and the chapters on object orientation in <span class="citation">Wickham (2019)</span>. The structure of the classes are presented in Figure . The abstract class <code>DoubleML</code> provides all methods for estimation and inference, for example the methods <code>fit()</code>, <code>bootstrap()</code>, <code>confint()</code>. All key components associated with estimation and inference are implemented in <code>DoubleML</code>, for example the sample splitting, the implementation of Algorithm 1 (DML1) and Algorithm 2 (DML2), the estimation of the causal parameters, and the computation of the scores <span class="math inline">\(\psi(W;\theta, \eta)\)</span>. Only the model-specific properties and methods are allocated at the classes <code>DoubleMLPLR</code> (implementing the PLR), <code>DoubleMLPLIV</code> (PLIV), <code>DoubleMLIRM</code> (IRM), and <code>DoubleMLIIVM</code> (IIVM). For example, each of the models has one or several Neyman-orthogonal score functions that are implemented for the specific child classes.</p>
<!-- ![OOP structure of the DoubleML package](figures/oop.pdf) -->
<p>The <code>DoubleMLData</code> class serves as the data-backend and implements the causal model of interest. The user is required to specify the roles of the variables in a data set at hand. Depending on the causal model considered, it is necessary to declare the dependent variable, the treatment variable(s), confounding variables(s), and, in the case of instrumental variable regression, one or multiple instruments. The data-backend can be initialized from a <code>data.table</code> <span class="citation">(Dowle and Srinivasan 2020)</span>. <code>DoubleML</code> provides wrappers to initialize from <code>data.frame</code> and <code>matrix</code> objects, as well.</p>
<p>Generally, all learners provided by the packages <code>mlr3</code>, <code>mlr3learners</code> and <code>mlr3extralearners</code> can be used for estimation of the nuisance functions of the structural models presented above. An interactive list of supported learners is available at the <code>mlr3extralearners</code> website. The <code>mlr3extralearners</code> package makes it possible to add new learners, as well. The performance of the double machine learning estimator <span class="math inline">\(\tilde\theta_0\)</span> will depend on the predictive quality of the used estimation method. Machine learning methods usually have several (hyper-)parameter that need to be adapted to a specific application. Tuning of model parameters can be either performed externally or internally. The latter is implemented in the method <code>tune()</code> and is further illustrated in an example in Section . Both cases build on the functionalities provided by the package <code>mlr3tuning</code>.</p>
<p>The flexible architecture of the <code>DoubleML</code> package allows users to modify the estimation procedure in many regards. Among others, users can provide customized sample splitting rules after initialization of the causal model via the method <code>set_sample_splitting()</code>. An example and the detailed requirements are provided in Section . Moreover, it is possible to adjust the Neyman-orthogonal score function by externally providing a customized function via the <code>score</code> option during initialization of the causal model object. A short example is presented in Section .</p>
<p>In this section, we will first demonstrate the use of <code>DoubleML</code> in a real-data example, which is based on data from the Pennsylvania Reemployment Bonus experiment <span class="citation">(Bilias 2000)</span>. This empirical example has been used in <span class="citation">Chernozhukov et al. (2018)</span>, as well. The goal in the empirical example is to estimate the causal parameter in a partially linear and an interactive regression model. We further provide a short example is given on how to perform simultaneous inference with <code>DoubleML</code>. Finally, we present results from a short simulation study as a brief assessment of the finite-sample performance of the implemented estimators.</p>
<p>We begin our real-data example by downloading the Pennsylvania Reemployment Bonus data set. To do so, we use the call (a connection to the internet is required).</p>
<p></p>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(DoubleML)</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Load data as data.table</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>dt_bonus <span class="ot">=</span> <span class="fu">fetch_bonus</span>(<span class="at">return_type =</span> <span class="st">&quot;data.table&quot;</span>)</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="co"># output suppressed for the sake of brevity</span></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>dt_bonus </span></code></pre></div>
<p> The data-backend <code>DoubleMLData</code> can be initialized from a <code>data.table</code> object by specifying the dependent variable <span class="math inline">\(Y\)</span> via a character in <code>y_col</code>, the treatment variable(s) <span class="math inline">\(D\)</span> in <code>d_cols</code>, and the confounders <span class="math inline">\(X\)</span> via <code>x_cols</code>. Moreover, in IV models, an instrument can be specified via <code>z_cols</code>. In the next step, we assign the roles to the variables in the data set: <code>y_col = &#39;inuidur1&#39;</code> serves as outcome variable <span class="math inline">\(Y\)</span>, the column <code>d_cols = &#39;tg&#39;</code> serves as treatment variable <span class="math inline">\(D\)</span> and the columns <code>x_cols</code> specify the confounders.</p>
<p></p>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>obj_dml_data_bonus <span class="ot">=</span> DoubleMLData<span class="sc">$</span><span class="fu">new</span>(dt_bonus,</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>                                      <span class="at">y_col =</span> <span class="st">&quot;inuidur1&quot;</span>,</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>                                      <span class="at">d_cols =</span> <span class="st">&quot;tg&quot;</span>,</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>                                      <span class="at">x_cols =</span> <span class="fu">c</span>(<span class="st">&quot;female&quot;</span>, <span class="st">&quot;black&quot;</span>, <span class="st">&quot;othrace&quot;</span>,</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>                                                 <span class="st">&quot;dep1&quot;</span>, <span class="st">&quot;dep2&quot;</span>, <span class="st">&quot;q2&quot;</span>, <span class="st">&quot;q3&quot;</span>,</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>                                                 <span class="st">&quot;q4&quot;</span>, <span class="st">&quot;q5&quot;</span>, <span class="st">&quot;q6&quot;</span>, <span class="st">&quot;agelt35&quot;</span>,</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>                                                 <span class="st">&quot;agegt54&quot;</span>, <span class="st">&quot;durable&quot;</span>, <span class="st">&quot;lusd&quot;</span>,</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>                                                 <span class="st">&quot;husd&quot;</span>))</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Print data backend: Lists main attributes and methods of a DoubleMLData object</span></span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>obj_dml_data_bonus</span></code></pre></div>
<pre><code>## ================= DoubleMLData Object ==================
## 
## 
## ------------------ Data summary      ------------------
## Outcome variable: inuidur1
## Treatment variable(s): tg
## Covariates: female, black, othrace, dep1, dep2, q2, q3, q4, q5, q6, agelt35, agegt54, durable, lusd, husd
## Instrument(s): 
## No. Observations: 5099</code></pre>
<p></p>
<p></p>
<div class="sourceCode" id="cb12"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Print data set (output suppressed)</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>obj_dml_data_bonus<span class="sc">$</span>data</span></code></pre></div>
<p></p>
<blockquote>
<p><strong>Remark 8</strong> (<em>Interface for <code>data.frame</code> and <code>matrix</code></em>) To initialize an instance of the class <code>DoubleMLData</code> from a <code>data.frame</code> or a collection of <code>matrix</code> objects, <code>DoubleML</code> provides the convenient wrappers <code>double_ml_data_from_data_frame()</code> and <code>double_ml_data_from_matrix()</code>. Examples can be found in the user guide and in the corresponding documentation.</p>
</blockquote>
<p>To initialize a PLR model, we have to provide a learner for each nuisance part in the model in Equation ()-(). In R, this is done by providing learners to the arguments <code>ml_m</code> for nuisance part <span class="math inline">\(m\)</span> and <code>ml_g</code> for nuisance part <span class="math inline">\(g\)</span>. We can pass a learner as instantiated in <code>mlr3</code> and <code>mlr3learners</code>, for example a random forest as provided by the R package <code>ranger</code> <span class="citation">(M. N. Wright and Ziegler 2017)</span>. Previous installation of <code>ranger</code> is required. Moreover, we can specify the score (allowed choices for PLR are <code>&quot;partialling out&quot;</code> or <code>&quot;IV-type&quot;</code>) and the algorithm via the option <code>dml_procedure</code> (allowed choices <code>&quot;dml1&quot;</code> and <code>&quot;dml2&quot;</code>) . Optionally, it is possible to change the number of folds used for sample splitting through <code>n_folds</code> and the number of repetitions via <code>n_rep</code>, if the sample splitting and estimation procedure should be repeated.</p>
<p></p>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">31415</span>) <span class="co"># required for reproducability of sample split</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>learner_g <span class="ot">=</span> <span class="fu">lrn</span>(<span class="st">&quot;regr.ranger&quot;</span>, <span class="at">num.trees =</span> <span class="dv">500</span>,</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>                <span class="at">min.node.size =</span> <span class="dv">2</span>, <span class="at">max.depth =</span> <span class="dv">5</span>)</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>learner_m <span class="ot">=</span> <span class="fu">lrn</span>(<span class="st">&quot;regr.ranger&quot;</span>, <span class="at">num.trees =</span> <span class="dv">500</span>,</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>                <span class="at">min.node.size =</span> <span class="dv">2</span>, <span class="at">max.depth =</span> <span class="dv">5</span>)</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>doubleml_bonus <span class="ot">=</span> DoubleMLPLR<span class="sc">$</span><span class="fu">new</span>(obj_dml_data_bonus, </span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>                                 <span class="at">ml_m =</span> learner_m, </span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>                                 <span class="at">ml_g =</span> learner_g, </span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>                                 <span class="at">score =</span> <span class="st">&quot;partialling out&quot;</span>,</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>                                 <span class="at">dml_procedure =</span> <span class="st">&quot;dml1&quot;</span>,</span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>                                 <span class="at">n_folds =</span> <span class="dv">5</span>, </span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>                                 <span class="at">n_rep =</span> <span class="dv">1</span>)</span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>doubleml_bonus</span></code></pre></div>
<pre><code>## ================= DoubleMLPLR Object ==================
## 
## 
## ------------------ Data summary      ------------------
## Outcome variable: inuidur1
## Treatment variable(s): tg
## Covariates: female, black, othrace, dep1, dep2, q2, q3, q4, q5, q6, agelt35, agegt54, durable, lusd, husd
## Instrument(s): 
## No. Observations: 5099
## 
## ------------------ Score &amp; algorithm ------------------
## Score function: partialling out
## DML algorithm: dml1
## 
## ------------------ Machine learner   ------------------
## ml_g: regr.ranger
## ml_m: regr.ranger
## 
## ------------------ Resampling        ------------------
## No. folds: 5
## No. repeated sample splits: 1
## Apply cross-fitting: TRUE
## 
## ------------------ Fit summary       ------------------
## </code></pre>
<pre><code>## fit() not yet called.</code></pre>
<p></p>
<p>To perform estimation, call the <code>fit()</code> method. The output can be summarized using the method <code>summary()</code>.</p>
<p></p>
<div class="sourceCode" id="cb16"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>doubleml_bonus<span class="sc">$</span><span class="fu">fit</span>()</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>doubleml_bonus<span class="sc">$</span><span class="fu">summary</span>()</span></code></pre></div>
<pre><code>## [1] &quot;Estimates and significance testing of the effect of target variables&quot;
##    Estimate. Std. Error t value Pr(&gt;|t|)  
## tg  -0.07438    0.03543  -2.099   0.0358 *
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p> Hence, we can reject the null hypothesis that <span class="math inline">\(\theta_{0, tg}=0\)</span> at the 5% significance level. The estimated coefficient and standard errors can be accessed via the attributes <code>coef</code> and <code>se</code> of the object <code>doubleml_bonus</code>.</p>
<p></p>
<div class="sourceCode" id="cb18"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>doubleml_bonus<span class="sc">$</span>coef</span></code></pre></div>
<pre><code>##          tg 
## -0.07438411</code></pre>
<div class="sourceCode" id="cb20"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>doubleml_bonus<span class="sc">$</span>se</span></code></pre></div>
<pre><code>##         tg 
## 0.03543316</code></pre>
<p></p>
<p>After completed estimation, we can access the resulting score <span class="math inline">\(\psi(W_i; \tilde{\theta}_0, \hat{\eta}_0)\)</span> or the components <span class="math inline">\(\psi_a(W_i; \hat{\eta}_0)\)</span> and <span class="math inline">\(\psi_b(W_i; \hat{\eta}_0)\)</span>. The estimated score for the first 5 observations can be obtained via.</p>
<p></p>
<div class="sourceCode" id="cb22"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Array with dim = c(n_obs, n_rep, n_treat)</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a><span class="co"># n_obs: number of observations in the data</span></span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a><span class="co"># n_rep: number of repetitions (sample splitting)</span></span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a><span class="co"># n_treat: number of treatment variables</span></span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>doubleml_bonus<span class="sc">$</span>psi[<span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>, <span class="dv">1</span>, <span class="dv">1</span>] </span></code></pre></div>
<pre><code>## [1] -0.2739454  0.7444154 -0.4509358  0.1813111 -0.3699474</code></pre>
<p> Similarly, the components of the score <span class="math inline">\(\psi_a(W_i; \hat{\eta}_0)\)</span> and <span class="math inline">\(\psi_b(W_i; \hat{\eta}_0)\)</span> are available as fields.</p>
<p></p>
<div class="sourceCode" id="cb24"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>doubleml_bonus<span class="sc">$</span>psi_a[<span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>, <span class="dv">1</span>, <span class="dv">1</span>] </span></code></pre></div>
<pre><code>## [1] -0.0981220 -0.1353987 -0.1276526 -0.4272341 -0.1126174</code></pre>
<div class="sourceCode" id="cb26"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>doubleml_bonus<span class="sc">$</span>psi_b[<span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>, <span class="dv">1</span>, <span class="dv">1</span>] </span></code></pre></div>
<pre><code>## [1] -0.2812441  0.7343439 -0.4604311  0.1495317 -0.3783243</code></pre>
<p> To construct a <span class="math inline">\((1-\alpha)\)</span> confidence interval, we use the <code>confint()</code> method.</p>
<p></p>
<div class="sourceCode" id="cb28"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>doubleml_bonus<span class="sc">$</span><span class="fu">confint</span>(<span class="at">level =</span> <span class="fl">0.95</span>)</span></code></pre></div>
<pre><code>##         2.5 %       97.5 %
## tg -0.1438318 -0.004936395</code></pre>
<p></p>
<p>The treatment variable <span class="math inline">\(D\)</span> in the Pennsylvania Reemployment Bonus example is binary. Accordingly, it is possible to estimate an IRM model. Since the IRM requires estimation of the propensity score <span class="math inline">\(\mathbb{P}(D|X)\)</span>, we have to specify a classifier for the nuisance part <span class="math inline">\(m_0\)</span>.</p>
<p></p>
<div class="sourceCode" id="cb30"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Classifier for propensity score</span></span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>learner_classif_m <span class="ot">=</span> <span class="fu">lrn</span>(<span class="st">&quot;classif.ranger&quot;</span>, <span class="at">num.trees =</span> <span class="dv">500</span>,</span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a>                        <span class="at">min.node.size =</span> <span class="dv">2</span>, <span class="at">max.depth =</span> <span class="dv">5</span>)</span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a>doubleml_irm_bonus <span class="ot">=</span> DoubleMLIRM<span class="sc">$</span><span class="fu">new</span>(obj_dml_data_bonus, </span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a>                                     <span class="at">ml_m =</span> learner_classif_m, </span>
<span id="cb30-7"><a href="#cb30-7" aria-hidden="true" tabindex="-1"></a>                                     <span class="at">ml_g =</span> learner_g, </span>
<span id="cb30-8"><a href="#cb30-8" aria-hidden="true" tabindex="-1"></a>                                     <span class="at">score =</span> <span class="st">&quot;ATE&quot;</span>,</span>
<span id="cb30-9"><a href="#cb30-9" aria-hidden="true" tabindex="-1"></a>                                     <span class="at">dml_procedure =</span> <span class="st">&quot;dml1&quot;</span>,</span>
<span id="cb30-10"><a href="#cb30-10" aria-hidden="true" tabindex="-1"></a>                                     <span class="at">n_folds =</span> <span class="dv">5</span>, </span>
<span id="cb30-11"><a href="#cb30-11" aria-hidden="true" tabindex="-1"></a>                                     <span class="at">n_rep =</span> <span class="dv">1</span>)</span>
<span id="cb30-12"><a href="#cb30-12" aria-hidden="true" tabindex="-1"></a><span class="co"># output suppressed</span></span>
<span id="cb30-13"><a href="#cb30-13" aria-hidden="true" tabindex="-1"></a>doubleml_irm_bonus</span></code></pre></div>
<pre><code>## fit() not yet called.</code></pre>
<p></p>
<p>To perform estimation, call the <code>fit()</code> method. The output can be summarized using the method <code>summary()</code>.</p>
<p></p>
<div class="sourceCode" id="cb32"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a>doubleml_irm_bonus<span class="sc">$</span><span class="fu">fit</span>()</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>doubleml_irm_bonus<span class="sc">$</span><span class="fu">summary</span>()</span></code></pre></div>
<pre><code>## [1] &quot;Estimates and significance testing of the effect of target variables&quot;
##    Estimate. Std. Error t value Pr(&gt;|t|)  
## tg  -0.07193    0.03554  -2.024    0.043 *
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p> The estimated coefficient is very similar to the estimate of the PLR model and our conclusions remain unchanged.</p>
<p>We consider a simulated example of a PLR model to illustrate the use of methods for simultaneous inference. First, we will generate a sparse linear model with only three variables having a non-zero effect on the dependent variable.</p>
<p></p>
<div class="sourceCode" id="cb34"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">3141</span>)</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>n_obs <span class="ot">=</span> <span class="dv">500</span></span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a>n_vars <span class="ot">=</span> <span class="dv">100</span></span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a>theta <span class="ot">=</span> <span class="fu">rep</span>(<span class="dv">3</span>, <span class="dv">3</span>)</span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a><span class="co"># generate matrix-like objects and use the corresponding wrapper</span></span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a>X <span class="ot">=</span> <span class="fu">matrix</span>(stats<span class="sc">::</span><span class="fu">rnorm</span>(n_obs <span class="sc">*</span> n_vars), <span class="at">nrow =</span> n_obs, <span class="at">ncol =</span> n_vars)</span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a>y <span class="ot">=</span> X[, <span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>, drop <span class="ot">=</span> <span class="cn">FALSE</span>] <span class="sc">%*%</span> theta  <span class="sc">+</span> stats<span class="sc">::</span><span class="fu">rnorm</span>(n_obs)</span>
<span id="cb34-9"><a href="#cb34-9" aria-hidden="true" tabindex="-1"></a>df <span class="ot">=</span> <span class="fu">data.frame</span>(y, X)</span></code></pre></div>
<p> We use the wrapper <code>double_ml_data_from_data_frame()</code> to specify a data-backend that assigns the first 10 columns of <span class="math inline">\(X\)</span> as treatment variables and declares the remaining columns as confounders.</p>
<p></p>
<div class="sourceCode" id="cb35"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a>doubleml_data <span class="ot">=</span> <span class="fu">double_ml_data_from_data_frame</span>(df,</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a>                                               <span class="at">y_col =</span> <span class="st">&quot;y&quot;</span>,</span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a>                                               <span class="at">d_cols =</span> <span class="fu">c</span>(<span class="st">&quot;X1&quot;</span>, <span class="st">&quot;X2&quot;</span>, <span class="st">&quot;X3&quot;</span>, </span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a>                                                          <span class="st">&quot;X4&quot;</span>, <span class="st">&quot;X5&quot;</span>, <span class="st">&quot;X6&quot;</span>, </span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a>                                                          <span class="st">&quot;X7&quot;</span>, <span class="st">&quot;X8&quot;</span>, <span class="st">&quot;X9&quot;</span>,</span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a>                                                          <span class="st">&quot;X10&quot;</span>))</span></code></pre></div>
<pre><code>## Set treatment variable d to X1.</code></pre>
<div class="sourceCode" id="cb37"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="co"># suppress output</span></span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a>doubleml_data</span></code></pre></div>
<p> A sparse setting suggests the use of the lasso learner. Here, we use the lasso estimator with cross-validated choice of the penalty parameter <span class="math inline">\(\lambda\)</span> as provided in the <code>glmnet</code> package for R <span class="citation">(Friedman, Hastie, and Tibshirani 2010)</span>.</p>
<p></p>
<div class="sourceCode" id="cb38"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="co"># output messages during fitting are suppressed</span></span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a>ml_g <span class="ot">=</span> <span class="fu">lrn</span>(<span class="st">&quot;regr.cv_glmnet&quot;</span>, <span class="at">s =</span> <span class="st">&quot;lambda.min&quot;</span>)</span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a>ml_m  <span class="ot">=</span> <span class="fu">lrn</span>(<span class="st">&quot;regr.cv_glmnet&quot;</span>, <span class="at">s =</span> <span class="st">&quot;lambda.min&quot;</span>)</span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true" tabindex="-1"></a>doubleml_plr <span class="ot">=</span> DoubleMLPLR<span class="sc">$</span><span class="fu">new</span>(doubleml_data, ml_g, ml_m)</span>
<span id="cb38-5"><a href="#cb38-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-6"><a href="#cb38-6" aria-hidden="true" tabindex="-1"></a>doubleml_plr<span class="sc">$</span><span class="fu">fit</span>()</span>
<span id="cb38-7"><a href="#cb38-7" aria-hidden="true" tabindex="-1"></a>doubleml_plr<span class="sc">$</span><span class="fu">summary</span>()</span></code></pre></div>
<pre><code>## [1] &quot;Estimates and significance testing of the effect of target variables&quot;
##     Estimate. Std. Error t value Pr(&gt;|t|)    
## X1   3.017802   0.046180  65.348   &lt;2e-16 ***
## X2   3.025812   0.042683  70.891   &lt;2e-16 ***
## X3   3.000914   0.045849  65.452   &lt;2e-16 ***
## X4  -0.034815   0.040955  -0.850   0.3953    
## X5   0.035118   0.048132   0.730   0.4656    
## X6   0.002171   0.044622   0.049   0.9612    
## X7  -0.036129   0.046798  -0.772   0.4401    
## X8   0.020361   0.044048   0.462   0.6439    
## X9  -0.019439   0.043180  -0.450   0.6526    
## X10  0.076180   0.043682   1.744   0.0812 .  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p> The multiplier bootstrap procedure can be executed using the <code>bootstrap()</code> method where the option <code>method</code> specifies the choice of the random pertubations and <code>n_rep_boot</code> the number of bootstrap repetitions.</p>
<p></p>
<div class="sourceCode" id="cb40"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a>doubleml_plr<span class="sc">$</span><span class="fu">bootstrap</span>(<span class="at">method =</span> <span class="st">&quot;normal&quot;</span>, <span class="at">n_rep_boot =</span> <span class="dv">1000</span>)</span></code></pre></div>
<p> The resulting bootstrapped coefficients and <span class="math inline">\(t\)</span>-statistics are available via the fields <code>boot_coef</code> and <code>boot_t_stat</code>. To construct a simultaneous confidence interval, we set the option <code>joint = TRUE</code> when calling the <code>confint()</code> method.</p>
<p></p>
<div class="sourceCode" id="cb41"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a>doubleml_plr<span class="sc">$</span><span class="fu">confint</span>(<span class="at">joint =</span> <span class="cn">TRUE</span>)</span></code></pre></div>
<pre><code>##           2.5 %     97.5 %
## X1   2.88766757 3.14793595
## X2   2.90553386 3.14609021
## X3   2.87171334 3.13011430
## X4  -0.15022399 0.08059423
## X5  -0.10051468 0.17075155
## X6  -0.12357302 0.12791441
## X7  -0.16800517 0.09574654
## X8  -0.10376590 0.14448792
## X9  -0.14111984 0.10224143
## X10 -0.04691574 0.19927524</code></pre>
<p></p>
<p>The correction of the <span class="math inline">\(p\)</span>-values of a joint hypotheses test on the considered causal parameters is implemented in the method <code>p_adjust()</code>. By default, the adjustment procedure specified in the option <code>method</code> is the Romano-Wolf stepdown procedure.</p>
<p></p>
<div class="sourceCode" id="cb43"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a>doubleml_plr<span class="sc">$</span><span class="fu">p_adjust</span>(<span class="at">method =</span> <span class="st">&quot;romano-wolf&quot;</span>)</span></code></pre></div>
<pre><code>##        Estimate.  pval
## X1   3.017801759 0.000
## X2   3.025812035 0.000
## X3   3.000913821 0.000
## X4  -0.034814877 0.942
## X5   0.035118435 0.942
## X6   0.002170694 0.961
## X7  -0.036129317 0.942
## X8   0.020361010 0.951
## X9  -0.019439209 0.951
## X10  0.076179750 0.451</code></pre>
<p> Alternatively, the correction methods provided in the <code>stats</code> function <code>p.adjust</code> can be applied, for example the Bonferroni, Bonferroni-Holm, or Benjamini-Hochberg correction. For example a Bonferroni correction could be performed by specifying <code>method = &quot;bonferroni&quot;</code>.</p>
<p></p>
<div class="sourceCode" id="cb45"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a>doubleml_plr<span class="sc">$</span><span class="fu">p_adjust</span>(<span class="at">method =</span> <span class="st">&quot;bonferroni&quot;</span>)</span></code></pre></div>
<pre><code>##        Estimate.      pval
## X1   3.017801759 0.0000000
## X2   3.025812035 0.0000000
## X3   3.000913821 0.0000000
## X4  -0.034814877 1.0000000
## X5   0.035118435 1.0000000
## X6   0.002170694 1.0000000
## X7  -0.036129317 1.0000000
## X8   0.020361010 1.0000000
## X9  -0.019439209 1.0000000
## X10  0.076179750 0.8116808</code></pre>
<p></p>
<p>The performance of the final double machine learning estimator depends on the predictive performance of the underlying ML method. First, we briefly show how externally tuned parameters can be passed to the learners in <code>DoubleML</code>. Second, it is demonstrated how the parameter tuning can be done internally by <code>DoubleML</code>.</p>
<p>Section 3 of the mlr3book <span class="citation">(Becker et al. 2021)</span> provides a step-by-step introduction to the powerful tuning functionalities of the <code>mlr3tuning</code> package. Accordingly, it is possible to manually reconstruct the <code>mlr3</code> regression and classification problems, which are internally handled in <code>DoubleML</code>, and to perform parameter tuning accordingly. One advantage of this procedure is that it allows users to fully exploit the powerful benchmarking and tuning tools of <code>mlr3</code> and <code>mlr3tuning</code>.</p>
<p>Consider the sparse regression example from above. We will briefly consider a setting where we explicitly set the parameter <span class="math inline">\(\lambda\)</span> for a <code>glmnet</code> estimator rather than using the interal cross-validated choice with <code>cv_glmnet</code>.</p>
<p>Suppose for simplicity, some external tuning procedure resulted in an optimal value of <span class="math inline">\(\lambda=0.1\)</span> for nuisance part <span class="math inline">\(m\)</span> and <span class="math inline">\(\lambda=0.09\)</span> for nuisance part <span class="math inline">\(g\)</span> for the first treatment variable and <span class="math inline">\(\lambda=0.095\)</span> and <span class="math inline">\(\lambda=0.085\)</span> for the second variable, respectively. After initialization of the model object, we can set the parameter values using the method <code>set_ml_nuisance_params()</code>.</p>
<p></p>
<div class="sourceCode" id="cb47"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a><span class="co"># output messages during fitting are suppressed</span></span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a>ml_g <span class="ot">=</span> <span class="fu">lrn</span>(<span class="st">&quot;regr.glmnet&quot;</span>)</span>
<span id="cb47-3"><a href="#cb47-3" aria-hidden="true" tabindex="-1"></a>ml_m  <span class="ot">=</span> <span class="fu">lrn</span>(<span class="st">&quot;regr.glmnet&quot;</span>)</span>
<span id="cb47-4"><a href="#cb47-4" aria-hidden="true" tabindex="-1"></a>doubleml_plr <span class="ot">=</span> DoubleMLPLR<span class="sc">$</span><span class="fu">new</span>(doubleml_data, ml_g, ml_m)</span></code></pre></div>
<p> To set the values, we have to specify the treatment variable and the nuisance part. If no values are set, the default values are used.</p>
<p></p>
<div class="sourceCode" id="cb48"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Note that variable names are overwritten by wrapper for matrix interface</span></span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a>doubleml_plr<span class="sc">$</span><span class="fu">set_ml_nuisance_params</span>(<span class="st">&quot;ml_m&quot;</span>, <span class="st">&quot;X1&quot;</span>,</span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a>                                    <span class="at">param =</span> <span class="fu">list</span>(<span class="st">&quot;lambda&quot;</span> <span class="ot">=</span> <span class="fl">0.1</span>))</span>
<span id="cb48-4"><a href="#cb48-4" aria-hidden="true" tabindex="-1"></a>doubleml_plr<span class="sc">$</span><span class="fu">set_ml_nuisance_params</span>(<span class="st">&quot;ml_g&quot;</span>, <span class="st">&quot;X1&quot;</span>,</span>
<span id="cb48-5"><a href="#cb48-5" aria-hidden="true" tabindex="-1"></a>                                    <span class="at">param =</span> <span class="fu">list</span>(<span class="st">&quot;lambda&quot;</span> <span class="ot">=</span> <span class="fl">0.09</span>))</span>
<span id="cb48-6"><a href="#cb48-6" aria-hidden="true" tabindex="-1"></a>doubleml_plr<span class="sc">$</span><span class="fu">set_ml_nuisance_params</span>(<span class="st">&quot;ml_m&quot;</span>, <span class="st">&quot;X2&quot;</span>,</span>
<span id="cb48-7"><a href="#cb48-7" aria-hidden="true" tabindex="-1"></a>                                    <span class="at">param =</span> <span class="fu">list</span>(<span class="st">&quot;lambda&quot;</span> <span class="ot">=</span> <span class="fl">0.095</span>))</span>
<span id="cb48-8"><a href="#cb48-8" aria-hidden="true" tabindex="-1"></a>doubleml_plr<span class="sc">$</span><span class="fu">set_ml_nuisance_params</span>(<span class="st">&quot;ml_g&quot;</span>, <span class="st">&quot;X2&quot;</span>,</span>
<span id="cb48-9"><a href="#cb48-9" aria-hidden="true" tabindex="-1"></a>                                    <span class="at">param =</span> <span class="fu">list</span>(<span class="st">&quot;lambda&quot;</span> <span class="ot">=</span> <span class="fl">0.085</span>))</span></code></pre></div>
<p> All externally specified parameters can be retrieved from the field <code>params</code>.</p>
<p></p>
<div class="sourceCode" id="cb49"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a><span class="co"># output omitted for the sake of brevity</span></span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(doubleml_plr<span class="sc">$</span>params)</span></code></pre></div>
<p></p>
<p></p>
<div class="sourceCode" id="cb50"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a>doubleml_plr<span class="sc">$</span><span class="fu">fit</span>()</span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a>doubleml_plr<span class="sc">$</span><span class="fu">summary</span>()</span></code></pre></div>
<pre><code>## [1] &quot;Estimates and significance testing of the effect of target variables&quot;
##     Estimate. Std. Error t value Pr(&gt;|t|)    
## X1   3.041094   0.060030  50.660   &lt;2e-16 ***
## X2   2.993916   0.054590  54.844   &lt;2e-16 ***
## X3   2.993419   0.055144  54.283   &lt;2e-16 ***
## X4  -0.035201   0.040637  -0.866    0.386    
## X5   0.021541   0.047569   0.453    0.651    
## X6  -0.006652   0.044715  -0.149    0.882    
## X7  -0.039650   0.046823  -0.847    0.397    
## X8   0.011146   0.044037   0.253    0.800    
## X9  -0.021342   0.043237  -0.494    0.622    
## X10  0.084426   0.043641   1.935    0.053 .  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p></p>
<p>An alternative to external tuning and parameter provisioning is to perform the tuning internally. The advantage of this approach is that users do not have to specify the underlying prediction problems manually. Instead, <code>DoubleML</code> uses the underlying data-backend to ensure that the machine learning methods are tuned for the specific model under consideration and, hence, to possibly avoid mistakes. We initialize our structural model object with the learner. At this stage, we do not specify any parameters.</p>
<p></p>
<div class="sourceCode" id="cb52"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a><span class="co"># load required packages for tuning</span></span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(paradox)</span>
<span id="cb52-3"><a href="#cb52-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(mlr3tuning)</span>
<span id="cb52-4"><a href="#cb52-4" aria-hidden="true" tabindex="-1"></a><span class="co"># set logger to omit messages during tuning and fitting</span></span>
<span id="cb52-5"><a href="#cb52-5" aria-hidden="true" tabindex="-1"></a>lgr<span class="sc">::</span><span class="fu">get_logger</span>(<span class="st">&quot;mlr3&quot;</span>)<span class="sc">$</span><span class="fu">set_threshold</span>(<span class="st">&quot;warn&quot;</span>)</span>
<span id="cb52-6"><a href="#cb52-6" aria-hidden="true" tabindex="-1"></a>lgr<span class="sc">::</span><span class="fu">get_logger</span>(<span class="st">&quot;bbotk&quot;</span>)<span class="sc">$</span><span class="fu">set_threshold</span>(<span class="st">&quot;warn&quot;</span>)</span>
<span id="cb52-7"><a href="#cb52-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-8"><a href="#cb52-8" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1234</span>)</span>
<span id="cb52-9"><a href="#cb52-9" aria-hidden="true" tabindex="-1"></a>ml_g <span class="ot">=</span> <span class="fu">lrn</span>(<span class="st">&quot;regr.glmnet&quot;</span>)</span>
<span id="cb52-10"><a href="#cb52-10" aria-hidden="true" tabindex="-1"></a>ml_m <span class="ot">=</span> <span class="fu">lrn</span>(<span class="st">&quot;regr.glmnet&quot;</span>)</span>
<span id="cb52-11"><a href="#cb52-11" aria-hidden="true" tabindex="-1"></a>doubleml_plr <span class="ot">=</span> DoubleMLPLR<span class="sc">$</span><span class="fu">new</span>(doubleml_data, ml_g, ml_m)</span></code></pre></div>
<p></p>
<p>To perform parameter tuning, we provide a grid of values used for evaluation for each of the nuisance parts. To set up a grid of values, we specify a named list with names corresponding to the learner names of the nuisance part (see method <code>learner_names()</code>). The elements in the list are objects of the class <code>ParamSet</code> of the <code>paradox</code> package <span class="citation">(Lang, Bischl, et al. 2020)</span>.</p>
<p></p>
<div class="sourceCode" id="cb53"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a>par_grids <span class="ot">=</span> <span class="fu">list</span>(</span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true" tabindex="-1"></a>  <span class="st">&quot;ml_g&quot;</span> <span class="ot">=</span> ParamSet<span class="sc">$</span><span class="fu">new</span>(<span class="fu">list</span>(</span>
<span id="cb53-3"><a href="#cb53-3" aria-hidden="true" tabindex="-1"></a>    ParamDbl<span class="sc">$</span><span class="fu">new</span>(<span class="st">&quot;lambda&quot;</span>, <span class="at">lower =</span> <span class="fl">0.05</span>, <span class="at">upper =</span> <span class="fl">0.1</span>))),</span>
<span id="cb53-4"><a href="#cb53-4" aria-hidden="true" tabindex="-1"></a>  <span class="st">&quot;ml_m&quot;</span> <span class="ot">=</span>  ParamSet<span class="sc">$</span><span class="fu">new</span>(<span class="fu">list</span>(</span>
<span id="cb53-5"><a href="#cb53-5" aria-hidden="true" tabindex="-1"></a>    ParamDbl<span class="sc">$</span><span class="fu">new</span>(<span class="st">&quot;lambda&quot;</span>, <span class="at">lower =</span> <span class="fl">0.05</span>, <span class="at">upper =</span> <span class="fl">0.1</span>))))</span></code></pre></div>
<p> The hyperparameter tuning is performed according to options passed through a named list <code>tune_settings</code>. The entries in the list specify options during parameter tuning with <code>mlr3tuning</code>:</p>
<ul>
<li><p><code>terminator</code> is a <code>bbotk::Terminator</code> object passed to <code>mlr3tuning</code> that manages the budget to solve the tuning problem.</p></li>
<li><p><code>algorithm</code> is an object of class <code>mlr3tuning::Tuner</code> and specifies the tuning algorithm. Alternatively, algorithm can be a <code>character()</code> that is used as an argument in the wrapper <code>mlr3tuning</code> call <code>tnr(algorithm)</code>. The <code>Tuner</code> class in <code>mlr3tuning</code> supports grid search, random search, generalized simulated annealing and non-linear optimization.</p></li>
<li><p><code>rsmp_tune</code> is an object of class <code>resampling</code> object that specifies the resampling method for evaluation, for example <code>rsmp(&quot;cv&quot;, folds = 5)</code> implements 5-fold cross-validation. <code>rsmp(&quot;holdout&quot;, ratio = 0.8)</code> implements an evaluation based on a hold-out sample that contains 20 percent of the observations. By default, 5-fold cross-validation is performed.</p></li>
<li><p><code>measure</code> is a named list containing the measures used for tuning of the nuisance components. The names of the entries must match the learner names (see method <code>learner_names()</code>). The entries in the list must either be objects of class <code>Measure</code> or keys passed to <code>msr()</code>. If <code>measure</code> is not provided by the user, the mean squared error is used for regression models and the classification error for binary outcomes, by default.</p></li>
</ul>
<p>In the next code chunk, the value of the parameter <span class="math inline">\(\lambda\)</span> is tuned via grid search in the range 0.05 to 0.1 at a resolution of 11.<a href="#fn5" class="footnote-ref" id="fnref5"><sup>5</sup></a> To evaluate the predictive performance in both nuisance parts, the cross-validated mean squared error is used.</p>
<p></p>
<div class="sourceCode" id="cb54"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Provide tune settings</span></span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a>tune_settings <span class="ot">=</span> <span class="fu">list</span>(<span class="at">terminator =</span> <span class="fu">trm</span>(<span class="st">&quot;evals&quot;</span>, <span class="at">n_evals =</span> <span class="dv">100</span>),</span>
<span id="cb54-3"><a href="#cb54-3" aria-hidden="true" tabindex="-1"></a>                     <span class="at">algorithm =</span> <span class="fu">tnr</span>(<span class="st">&quot;grid_search&quot;</span>, <span class="at">resolution =</span> <span class="dv">11</span>),</span>
<span id="cb54-4"><a href="#cb54-4" aria-hidden="true" tabindex="-1"></a>                     <span class="at">rsmp_tune =</span> <span class="fu">rsmp</span>(<span class="st">&quot;cv&quot;</span>, <span class="at">folds =</span> <span class="dv">5</span>),</span>
<span id="cb54-5"><a href="#cb54-5" aria-hidden="true" tabindex="-1"></a>                     <span class="at">measure =</span> <span class="fu">list</span>(<span class="st">&quot;ml_g&quot;</span> <span class="ot">=</span> <span class="fu">msr</span>(<span class="st">&quot;regr.mse&quot;</span>),</span>
<span id="cb54-6"><a href="#cb54-6" aria-hidden="true" tabindex="-1"></a>                                    <span class="st">&quot;ml_m&quot;</span> <span class="ot">=</span> <span class="fu">msr</span>(<span class="st">&quot;regr.mse&quot;</span>)))</span></code></pre></div>
<p> With these parameters we can run the tuning by calling the <code>tune</code> method for <code>DoubleML</code> objects.</p>
<p></p>
<div class="sourceCode" id="cb55"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a><span class="co"># execution might take around 50 seconds</span></span>
<span id="cb55-2"><a href="#cb55-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Tune</span></span>
<span id="cb55-3"><a href="#cb55-3" aria-hidden="true" tabindex="-1"></a>doubleml_plr<span class="sc">$</span><span class="fu">tune</span>(<span class="at">param_set =</span> par_grids, <span class="at">tune_settings =</span> tune_settings)</span>
<span id="cb55-4"><a href="#cb55-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-5"><a href="#cb55-5" aria-hidden="true" tabindex="-1"></a><span class="co"># output omitted for the sake of brevity, available in the Appendix</span></span>
<span id="cb55-6"><a href="#cb55-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-7"><a href="#cb55-7" aria-hidden="true" tabindex="-1"></a><span class="co"># acces tuning results for target variable &quot;X1&quot;</span></span>
<span id="cb55-8"><a href="#cb55-8" aria-hidden="true" tabindex="-1"></a>doubleml_plr<span class="sc">$</span>tuning_res<span class="sc">$</span>X1</span>
<span id="cb55-9"><a href="#cb55-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-10"><a href="#cb55-10" aria-hidden="true" tabindex="-1"></a><span class="co"># tuned parameters</span></span>
<span id="cb55-11"><a href="#cb55-11" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(doubleml_plr<span class="sc">$</span>params)</span></code></pre></div>
<p></p>
<p></p>
<div class="sourceCode" id="cb56"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a><span class="co"># estimate model and summary</span></span>
<span id="cb56-2"><a href="#cb56-2" aria-hidden="true" tabindex="-1"></a>doubleml_plr<span class="sc">$</span><span class="fu">fit</span>()</span>
<span id="cb56-3"><a href="#cb56-3" aria-hidden="true" tabindex="-1"></a>doubleml_plr<span class="sc">$</span><span class="fu">summary</span>()</span></code></pre></div>
<pre><code>## [1] &quot;Estimates and significance testing of the effect of target variables&quot;
##     Estimate. Std. Error t value Pr(&gt;|t|)    
## X1   3.028980   0.059701  50.736   &lt;2e-16 ***
## X2   3.008650   0.054301  55.407   &lt;2e-16 ***
## X3   2.960571   0.053082  55.773   &lt;2e-16 ***
## X4  -0.037859   0.040976  -0.924   0.3555    
## X5   0.030018   0.047880   0.627   0.5307    
## X6   0.003451   0.044419   0.078   0.9381    
## X7  -0.025875   0.046936  -0.551   0.5814    
## X8   0.022008   0.044172   0.498   0.6183    
## X9  -0.014251   0.043765  -0.326   0.7447    
## X10  0.088653   0.043691   2.029   0.0424 *  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p></p>
<p>By default, the parameter tuning is performed on the whole sample, for example in the case of <span class="math inline">\(K_{tune}\)</span>-fold cross-validation, the entire sample is split into <span class="math inline">\(K_{tune}\)</span> folds for evaluation of the cross-validated error. Alternatively, each of the <span class="math inline">\(K\)</span> folds used in the cross-fitting procedure could be split up into <span class="math inline">\(K_{tune}\)</span> subfolds that are then used for evaluation of the candidate models. As a result, the choice of the tuned parameters will be fold-specific. To perform fold-specific tuning, users can set the option <code>tune_on_folds = TRUE</code> when calling the method <code>tune()</code>.</p>
<p>The flexible architecture of the <code>DoubleML</code> package allows users to modify the estimation procedure in many regards. We will shortly present two examples on how users can adjust the double machine learning framework to their needs in terms of the sample splitting procedure and the score function.</p>
<p>By default, <code>DoubleML</code> performs cross-fitting as presented in Algorithms 1 and 2. Alternatively, all implemented models allow a partition to be provided externally via the method <code>set_sample_splitting()</code>. Note that by setting <code>draw_sample_splitting = FALSE</code> one can prevent that a partition is drawn during initialization of the model object. The following calls are equivalent. In the first sample code, we use the standard interface and draw the sample-splitting with <span class="math inline">\(K=4\)</span> folds during initialization of the <code>DoubleMLPLR</code> object.</p>
<p></p>
<div class="sourceCode" id="cb58"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a><span class="co"># First generate some data, ml learners and a data-backend</span></span>
<span id="cb58-2"><a href="#cb58-2" aria-hidden="true" tabindex="-1"></a>learner <span class="ot">=</span> <span class="fu">lrn</span>(<span class="st">&quot;regr.ranger&quot;</span>, <span class="at">num.trees =</span> <span class="dv">100</span>, <span class="at">mtry =</span> <span class="dv">20</span>,</span>
<span id="cb58-3"><a href="#cb58-3" aria-hidden="true" tabindex="-1"></a>              <span class="at">min.node.size =</span> <span class="dv">2</span>, <span class="at">max.depth =</span> <span class="dv">5</span>)</span>
<span id="cb58-4"><a href="#cb58-4" aria-hidden="true" tabindex="-1"></a>ml_g <span class="ot">=</span> learner</span>
<span id="cb58-5"><a href="#cb58-5" aria-hidden="true" tabindex="-1"></a>ml_m <span class="ot">=</span> learner</span>
<span id="cb58-6"><a href="#cb58-6" aria-hidden="true" tabindex="-1"></a>data <span class="ot">=</span> <span class="fu">make_plr_CCDDHNR2018</span>(<span class="at">alpha =</span> <span class="fl">0.5</span>, <span class="at">n_obs =</span> <span class="dv">100</span>,</span>
<span id="cb58-7"><a href="#cb58-7" aria-hidden="true" tabindex="-1"></a>                            <span class="at">return_type =</span> <span class="st">&quot;data.table&quot;</span>)</span>
<span id="cb58-8"><a href="#cb58-8" aria-hidden="true" tabindex="-1"></a>doubleml_data <span class="ot">=</span> DoubleMLData<span class="sc">$</span><span class="fu">new</span>(data,</span>
<span id="cb58-9"><a href="#cb58-9" aria-hidden="true" tabindex="-1"></a>                                 <span class="at">y_col =</span> <span class="st">&quot;y&quot;</span>,</span>
<span id="cb58-10"><a href="#cb58-10" aria-hidden="true" tabindex="-1"></a>                                 <span class="at">d_cols =</span> <span class="st">&quot;d&quot;</span>)</span></code></pre></div>
<p></p>
<p></p>
<div class="sourceCode" id="cb59"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">314</span>)</span>
<span id="cb59-2"><a href="#cb59-2" aria-hidden="true" tabindex="-1"></a>doubleml_plr_internal <span class="ot">=</span> DoubleMLPLR<span class="sc">$</span><span class="fu">new</span>(doubleml_data, ml_g, ml_m, <span class="at">n_folds =</span> <span class="dv">4</span>)</span>
<span id="cb59-3"><a href="#cb59-3" aria-hidden="true" tabindex="-1"></a>doubleml_plr_internal<span class="sc">$</span><span class="fu">fit</span>()</span>
<span id="cb59-4"><a href="#cb59-4" aria-hidden="true" tabindex="-1"></a>doubleml_plr_internal<span class="sc">$</span><span class="fu">summary</span>()</span></code></pre></div>
<pre><code>## [1] &quot;Estimates and significance testing of the effect of target variables&quot;
##   Estimate. Std. Error t value Pr(&gt;|t|)    
## d    0.4892     0.1024   4.776 1.79e-06 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p></p>
<p>In the second sample code, we manually specify a sampling scheme using the <code>mlr3::Resampling</code> class. Alternatively, users can provide a nested list that has the following structure:</p>
<ul>
<li>The length of the outer list must match with the desired number of repetitions of the sample-splitting, i.e., <code>n_rep</code>.</li>
<li>The inner list is a named list of length 2 specifying the <code>test_ids</code> and <code>train_ids</code>. The named entries <code>test_ids</code> and <code>train_ids</code> are lists of the same length.
<ul>
<li><code>train_ids</code> is a list of length <code>n_folds</code> that specifies the indices of the observations used for model fitting in each fold.</li>
<li><code>test_ids</code> is a list of length <code>n_folds</code> that specifies the indices of the observations used for calculation of the score in each fold.</li>
</ul></li>
</ul>
<p></p>
<div class="sourceCode" id="cb61"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a>doubleml_plr_external <span class="ot">=</span> DoubleMLPLR<span class="sc">$</span><span class="fu">new</span>(doubleml_data, ml_g, ml_m,</span>
<span id="cb61-2"><a href="#cb61-2" aria-hidden="true" tabindex="-1"></a>                                        <span class="at">draw_sample_splitting =</span> <span class="cn">FALSE</span>)</span>
<span id="cb61-3"><a href="#cb61-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-4"><a href="#cb61-4" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">314</span>)</span>
<span id="cb61-5"><a href="#cb61-5" aria-hidden="true" tabindex="-1"></a><span class="co"># set up a task and cross-validation resampling scheme in mlr3</span></span>
<span id="cb61-6"><a href="#cb61-6" aria-hidden="true" tabindex="-1"></a>my_task <span class="ot">=</span> Task<span class="sc">$</span><span class="fu">new</span>(<span class="st">&quot;help task&quot;</span>, <span class="st">&quot;regr&quot;</span>, data)</span>
<span id="cb61-7"><a href="#cb61-7" aria-hidden="true" tabindex="-1"></a>my_sampling <span class="ot">=</span> <span class="fu">rsmp</span>(<span class="st">&quot;cv&quot;</span>, <span class="at">folds =</span> <span class="dv">4</span>)<span class="sc">$</span><span class="fu">instantiate</span>(my_task)</span>
<span id="cb61-8"><a href="#cb61-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-9"><a href="#cb61-9" aria-hidden="true" tabindex="-1"></a>train_ids <span class="ot">=</span> <span class="fu">lapply</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>, <span class="cf">function</span>(x) my_sampling<span class="sc">$</span><span class="fu">train_set</span>(x))</span>
<span id="cb61-10"><a href="#cb61-10" aria-hidden="true" tabindex="-1"></a>test_ids <span class="ot">=</span> <span class="fu">lapply</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>, <span class="cf">function</span>(x) my_sampling<span class="sc">$</span><span class="fu">test_set</span>(x))</span>
<span id="cb61-11"><a href="#cb61-11" aria-hidden="true" tabindex="-1"></a>smpls <span class="ot">=</span> <span class="fu">list</span>(<span class="fu">list</span>(<span class="at">train_ids =</span> train_ids, <span class="at">test_ids =</span> test_ids))</span>
<span id="cb61-12"><a href="#cb61-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-13"><a href="#cb61-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Structure of the specified sampling scheme </span></span>
<span id="cb61-14"><a href="#cb61-14" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(smpls)</span></code></pre></div>
<pre><code>## List of 1
##  $ :List of 2
##   ..$ train_ids:List of 4
##   .. ..$ : int [1:75] 1 7 11 18 19 20 21 31 32 37 ...
##   .. ..$ : int [1:75] 10 15 16 22 26 35 38 40 41 46 ...
##   .. ..$ : int [1:75] 10 15 16 22 26 35 38 40 41 46 ...
##   .. ..$ : int [1:75] 10 15 16 22 26 35 38 40 41 46 ...
##   ..$ test_ids :List of 4
##   .. ..$ : int [1:25] 10 15 16 22 26 35 38 40 41 46 ...
##   .. ..$ : int [1:25] 1 7 11 18 19 20 21 31 32 37 ...
##   .. ..$ : int [1:25] 3 5 6 8 17 24 25 28 29 34 ...
##   .. ..$ : int [1:25] 2 4 9 12 13 14 23 27 30 33 ...</code></pre>
<div class="sourceCode" id="cb63"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit model</span></span>
<span id="cb63-2"><a href="#cb63-2" aria-hidden="true" tabindex="-1"></a>doubleml_plr_external<span class="sc">$</span><span class="fu">set_sample_splitting</span>(smpls)</span>
<span id="cb63-3"><a href="#cb63-3" aria-hidden="true" tabindex="-1"></a>doubleml_plr_external<span class="sc">$</span><span class="fu">fit</span>()</span>
<span id="cb63-4"><a href="#cb63-4" aria-hidden="true" tabindex="-1"></a>doubleml_plr_external<span class="sc">$</span><span class="fu">summary</span>()</span></code></pre></div>
<pre><code>## [1] &quot;Estimates and significance testing of the effect of target variables&quot;
##   Estimate. Std. Error t value Pr(&gt;|t|)    
## d    0.4892     0.1024   4.776 1.79e-06 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p></p>
<p>Setting the option <code>apply_cross_fitting = FALSE</code> at the instantiation of the causal model allows double machine learning being performed without cross-fitting. It results in randomly splitting the sample into two parts. The first half of the data is used for the estimation of the nuisance models with the machine learning methods and the second half for estimating the causal parameter, i.e., solution of the score. Note that cross-fitting performs well empirically and is recommended to remove bias induced by overfitting. Moreover, cross-fitting allows to exploit full efficiency: Every fold is used once for training the ML methods and once for estimation of the score <span class="citation">(Chernozhukov et al. 2018, 6)</span>. A short example on the efficiency gains associated with cross-fitting is provided in Section .</p>
<p>Users may want to adjust the score function <span class="math inline">\(\psi(W;\theta_0, \eta_0)\)</span>, for example, to adjust the DML estimators in terms of a re-weighting. An alternative to the choices provided in <code>DoubleML</code> is to pass a function via the argument <code>score</code> during initialization of the model object. The following examples are equivalent. In the first example, we use the score option <code>&quot;partialling out&quot;</code> for the PLR model whereas in the second case, we explicitly provide a function that implements the same score. The arguments used in the function refer to the internal objects that implement the theoretical quantities in Equation ().</p>
<p></p>
<div class="sourceCode" id="cb65"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a><span class="co"># use score &quot;partialling out&quot;</span></span>
<span id="cb65-2"><a href="#cb65-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">314</span>)</span>
<span id="cb65-3"><a href="#cb65-3" aria-hidden="true" tabindex="-1"></a>doubleml_plr_partout <span class="ot">=</span> DoubleMLPLR<span class="sc">$</span><span class="fu">new</span>(doubleml_data, ml_g, ml_m,</span>
<span id="cb65-4"><a href="#cb65-4" aria-hidden="true" tabindex="-1"></a>                                       <span class="at">score =</span> <span class="st">&quot;partialling out&quot;</span>)</span>
<span id="cb65-5"><a href="#cb65-5" aria-hidden="true" tabindex="-1"></a>doubleml_plr_partout<span class="sc">$</span><span class="fu">fit</span>()</span>
<span id="cb65-6"><a href="#cb65-6" aria-hidden="true" tabindex="-1"></a>doubleml_plr_partout<span class="sc">$</span><span class="fu">summary</span>()</span></code></pre></div>
<pre><code>## [1] &quot;Estimates and significance testing of the effect of target variables&quot;
##   Estimate. Std. Error t value Pr(&gt;|t|)    
## d    0.5108     0.0959   5.326    1e-07 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p></p>
<p>We define the function that implements the same score and specify the argument <code>score</code> accordingly. The function must return a named list with entries <code>psi_a</code> and <code>psi_b</code> to pass values for computation of the score.</p>
<p></p>
<div class="sourceCode" id="cb67"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb67-1"><a href="#cb67-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Here: </span></span>
<span id="cb67-2"><a href="#cb67-2" aria-hidden="true" tabindex="-1"></a><span class="co"># y: dependent variable</span></span>
<span id="cb67-3"><a href="#cb67-3" aria-hidden="true" tabindex="-1"></a><span class="co"># d: treatment variable</span></span>
<span id="cb67-4"><a href="#cb67-4" aria-hidden="true" tabindex="-1"></a><span class="co"># g_hat: predicted values from regression of Y on X&#39;s</span></span>
<span id="cb67-5"><a href="#cb67-5" aria-hidden="true" tabindex="-1"></a><span class="co"># m_hat: predicted values from regression of D on X&#39;s</span></span>
<span id="cb67-6"><a href="#cb67-6" aria-hidden="true" tabindex="-1"></a><span class="co"># smpls: sample split under consideration, can be ignored in this example</span></span>
<span id="cb67-7"><a href="#cb67-7" aria-hidden="true" tabindex="-1"></a>score_manual <span class="ot">=</span> <span class="cf">function</span>(y, d, g_hat, m_hat, smpls) {</span>
<span id="cb67-8"><a href="#cb67-8" aria-hidden="true" tabindex="-1"></a>  resid_y <span class="ot">=</span> y <span class="sc">-</span> g_hat</span>
<span id="cb67-9"><a href="#cb67-9" aria-hidden="true" tabindex="-1"></a>  resid_d <span class="ot">=</span> d <span class="sc">-</span> m_hat</span>
<span id="cb67-10"><a href="#cb67-10" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb67-11"><a href="#cb67-11" aria-hidden="true" tabindex="-1"></a>  psi_a <span class="ot">=</span> <span class="sc">-</span><span class="dv">1</span> <span class="sc">*</span> resid_d <span class="sc">*</span> resid_d</span>
<span id="cb67-12"><a href="#cb67-12" aria-hidden="true" tabindex="-1"></a>  psi_b <span class="ot">=</span> resid_d <span class="sc">*</span> resid_y</span>
<span id="cb67-13"><a href="#cb67-13" aria-hidden="true" tabindex="-1"></a>  psis <span class="ot">=</span> <span class="fu">list</span>(<span class="at">psi_a =</span> psi_a, <span class="at">psi_b =</span> psi_b)</span>
<span id="cb67-14"><a href="#cb67-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(psis)</span>
<span id="cb67-15"><a href="#cb67-15" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p></p>
<p></p>
<div class="sourceCode" id="cb68"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb68-1"><a href="#cb68-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">314</span>)</span>
<span id="cb68-2"><a href="#cb68-2" aria-hidden="true" tabindex="-1"></a>doubleml_plr_manual <span class="ot">=</span> DoubleMLPLR<span class="sc">$</span><span class="fu">new</span>(doubleml_data, ml_g, ml_m,</span>
<span id="cb68-3"><a href="#cb68-3" aria-hidden="true" tabindex="-1"></a>                                      <span class="at">score =</span> score_manual)</span>
<span id="cb68-4"><a href="#cb68-4" aria-hidden="true" tabindex="-1"></a>doubleml_plr_manual<span class="sc">$</span><span class="fu">fit</span>()</span>
<span id="cb68-5"><a href="#cb68-5" aria-hidden="true" tabindex="-1"></a>doubleml_plr_manual<span class="sc">$</span><span class="fu">summary</span>()</span></code></pre></div>
<pre><code>## [1] &quot;Estimates and significance testing of the effect of target variables&quot;
##   Estimate. Std. Error t value Pr(&gt;|t|)    
## d    0.5108     0.0959   5.326    1e-07 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p></p>
<p>To illustrate the validity of the implemented double machine learning estimators, we perform a brief simulation study.</p>
<p>As mentioned in Section  the use of the cross-fitting Algorithms 1 (DML1) and 2 (DML2) makes it possible to use sample splitting and exploit full efficiency at the same time. To illustrate the superior performance due to cross-fitting, we compare the double machine learning estimator with and without a cross-fitting procedure in the simulation setting that was presented in . Figure  illustrates that efficiency gains can be achieved if the role of the random partitions is swapped in the estimation procedure. Using cross-fitting makes it possible to obtain smaller standard errors for the DML estimator: The empirical distribution of the double machine learning estimator that is based on the cross-fitting Algorithm 2 (DML2) exhibits a more pronounced concentration around zero.</p>
<p>We provide simulation results for double machine learning estimators in the presented key causal models in Figure . In a replication of the simulation example in Section , we show that the confidence intervals for the DML estimator in the partially linear regression model achieves an empirical coverage close to the specified level of <span class="math inline">\(1-\alpha=0.95\)</span>. The estimator is, again, based on a random forest learner. The corresponding results are presented in the top-left panel of Figure .</p>
<p>In a simulated example of a PLIV model, the DML confidence interval that is based on a lasso learner (<code>regr.cv_glmnet</code> of <code>mlr3</code>) achieves a coverage of 94.4%. The underlying data generating process is based on a setting considered in <span class="citation">Chernozhukov, Hansen, and Spindler (2015a)</span> with one instrumental variable. Moreover for simulations of the IRM model, we make use of a DGP of <span class="citation">Belloni et al. (2017)</span>. The DGP for the IIVM is inspired by a simulation run in <span class="citation">Farbmacher, Guber, and Klaassen (2020)</span>. We present the formal DGPs in the Appendix. To perform estimation of the nuisance parts in the interactive models, we employ the regression and classification predictors <code>regr.cv_glmnet</code> and <code>classif.cv_glmnet</code> as provided by the <code>mlr3</code> package. In all cases, we employ the cross-validated <code>lambda.min</code> choice of the penalty parameter with five folds, in other words, that <span class="math inline">\(\lambda\)</span> value that minimizes the cross-validated mean squared error. Figure  shows that the empirical distribution of the centered estimators as obtained in finite sample settings is relatively well-approximated by a normal distribution. In all models the empirical coverage that is achieved by the constructed confidence bands is close to the nominal level.</p>
<p>To verify the finite-sample performance of the implemented methods for simultaneous inference, we perform a small simulation study in a regression setup which is similar as the one used in <span class="citation">Bach, Chernozhukov, and Spindler (2018)</span>. We would like to perform valid simultaneous inference on the coefficients <span class="math inline">\(\theta\)</span> in the regression model <span class="math display">\[\begin{align} \label{regsim}
y_i = \beta_0 + d_i&#39;\theta + \varepsilon_i , \quad \quad i = 1,\ldots, n,
\end{align}\]</span> with <span class="math inline">\(n=1000\)</span> and <span class="math inline">\(p_1=42\)</span> regressors. The errors <span class="math inline">\(\varepsilon_i\)</span> are normally distributed with <span class="math inline">\(\varepsilon_i \sim N(0,\sigma^2)\)</span> and variance <span class="math inline">\(\sigma^2=3\)</span>. The regressors <span class="math inline">\(d_i\)</span> are generated by a joint normal distribution <span class="math inline">\(d_i \sim N(\mu, \Sigma)\)</span> with <span class="math inline">\(\mu = \mathbf{0}\)</span> and <span class="math inline">\(\Sigma_{j,k} = 0.5^{|j-k|}\)</span>. The model is sparse in that only the first <span class="math inline">\(s=12\)</span> regressors have a non-zero effect on outcome <span class="math inline">\(y_i\)</span>. The <span class="math inline">\(p_1\)</span> coefficients <span class="math inline">\(\theta_1, \ldots, \theta_{p_1}\)</span> are generated as <span class="math display">\[\begin{align*}
\theta_j= \min\left\{\frac{\theta^{\max}}{j^{a}}, \theta^{\min} \right\},
\end{align*}\]</span> for <span class="math inline">\(j=1, \ldots, s\)</span> with <span class="math inline">\(\theta^{\max}=9\)</span>, <span class="math inline">\(\theta^{min}=0.75\)</span>, and <span class="math inline">\(a=0.99\)</span>. All other coefficients have values exactly equal to <span class="math inline">\(0\)</span>. Estimation of the nuisance components has been performed by using the lasso as provided by <code>regr.cv_glmnet</code> in <code>mlr3</code>.</p>
<p>We report the empirical coverage as achieved by a joint <span class="math inline">\((1-\alpha)\)</span>-confidence interval for all <span class="math inline">\(p_1=42\)</span> coefficients and the realized family-wise error rate of the implemented <span class="math inline">\(p\)</span>-value adjustments in <span class="math inline">\(R=500\)</span> repetitions in Table . The finite sample performance of the Romano-Wolf stepdown procedure that is based on the multiplier bootstrap as well as the classical Bonferroni and Bonferroni-Holm correction are evaluated. Table  shows that all methods achieve an empirical FWER close to the specified level of <span class="math inline">\(\alpha = 0.1\)</span>. In all cases, the double machine learning estimators reject all 12 false null hypotheses in every repetition.</p>
<p>In this paper, we provide an overview on the key ingredients and the major structure of the double/debiased machine learning framework as established in <span class="citation">Chernozhukov et al. (2018)</span> together with an overview on a collection of structural models. Moreover, we introduce the R package <code>DoubleML</code> that serves as an implementation of the double machine learning approach. A brief simulation study provides insights on the finite sample performance of the double machine learning estimator in the key causal models.</p>
<p>The structure of <code>DoubleML</code> is intended to be flexible with regard to the implemented structural models, the resampling scheme, the machine learning methods and the underlying algorithm, as well as the Neyman-orthogonal scores considered. By providing the R package <code>DoubleML</code> together with its Python twin <span class="citation">(Bach et al. 2021)</span>, we hope to make double machine learning more accessible to users in practice. Finally, we would like to encourage users to add new structural models, scores and functionalities to the package.</p>
<hr />
<p>This work was funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) – Project Number 431701914.</p>
<div style="page-break-after: always;"></div>
<p>The simulation study has been run on a x86_64-w64-mingw32/x64 (64-bit) (Windows 10 x64 (build 19041)) system using R version 3.6.3 (2020-02-29). The following packages have been used for estimation:</p>
<ul>
<li><code>DoubleML</code>, version 0.1.2,</li>
<li><code>data.table</code>, version 1.13.2,</li>
<li><code>mlr3</code>, version 0.8.0,</li>
<li><code>mlr3tuning</code>, version 0.6.0,</li>
<li><code>mlr3learners</code>, version 0.4.2,</li>
<li><code>glmnet</code>, version 3.0.2,</li>
<li><code>ranger</code>, version 0.12.1,</li>
<li><code>paradox</code>, version 0.7.0</li>
<li><code>foreach</code>, version 1.5.1.</li>
</ul>
<p></p>
<p></p>
<div class="sourceCode" id="cb70"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb70-1"><a href="#cb70-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(DoubleML)</span>
<span id="cb70-2"><a href="#cb70-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Load data as data.table</span></span>
<span id="cb70-3"><a href="#cb70-3" aria-hidden="true" tabindex="-1"></a>dt_bonus <span class="ot">=</span> <span class="fu">fetch_bonus</span>(<span class="at">return_type =</span> <span class="st">&quot;data.table&quot;</span>)</span>
<span id="cb70-4"><a href="#cb70-4" aria-hidden="true" tabindex="-1"></a>dt_bonus</span></code></pre></div>
<pre><code>##       inuidur1 female black othrace dep1 dep2 q2 q3 q4 q5 q6 agelt35 agegt54
##    1: 2.890372      0     0       0    0    1  0  0  0  1  0       0       0
##    2: 0.000000      0     0       0    0    0  0  0  0  1  0       0       0
##    3: 3.295837      0     0       0    0    0  0  0  1  0  0       0       0
##    4: 2.197225      0     0       0    0    0  0  1  0  0  0       1       0
##    5: 3.295837      0     0       0    1    0  0  0  0  1  0       0       1
##   ---                                                                       
## 5095: 2.302585      0     0       0    0    0  0  1  0  0  0       1       0
## 5096: 1.386294      0     0       0    0    1  1  0  0  0  0       0       0
## 5097: 2.197225      0     0       0    0    1  1  0  0  0  0       1       0
## 5098: 1.386294      0     0       0    0    0  0  0  0  1  0       0       1
## 5099: 3.295837      0     0       0    0    0  0  0  1  0  0       0       1
##       durable lusd husd tg
##    1:       0    0    1  0
##    2:       0    1    0  0
##    3:       0    1    0  0
##    4:       0    0    0  1
##    5:       1    1    0  0
##   ---                     
## 5095:       0    0    0  1
## 5096:       0    0    0  1
## 5097:       0    1    0  0
## 5098:       0    0    0  1
## 5099:       1    1    0  0</code></pre>
<p></p>
<p></p>
<div class="sourceCode" id="cb72"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb72-1"><a href="#cb72-1" aria-hidden="true" tabindex="-1"></a>obj_dml_data_bonus <span class="ot">=</span> DoubleMLData<span class="sc">$</span><span class="fu">new</span>(dt_bonus,</span>
<span id="cb72-2"><a href="#cb72-2" aria-hidden="true" tabindex="-1"></a>                                      <span class="at">y_col =</span> <span class="st">&quot;inuidur1&quot;</span>,</span>
<span id="cb72-3"><a href="#cb72-3" aria-hidden="true" tabindex="-1"></a>                                      <span class="at">d_cols =</span> <span class="st">&quot;tg&quot;</span>,</span>
<span id="cb72-4"><a href="#cb72-4" aria-hidden="true" tabindex="-1"></a>                                      <span class="at">x_cols =</span> <span class="fu">c</span>(<span class="st">&quot;female&quot;</span>, <span class="st">&quot;black&quot;</span>, <span class="st">&quot;othrace&quot;</span>,</span>
<span id="cb72-5"><a href="#cb72-5" aria-hidden="true" tabindex="-1"></a>                                                 <span class="st">&quot;dep1&quot;</span>, <span class="st">&quot;dep2&quot;</span>, <span class="st">&quot;q2&quot;</span>, <span class="st">&quot;q3&quot;</span>,</span>
<span id="cb72-6"><a href="#cb72-6" aria-hidden="true" tabindex="-1"></a>                                                 <span class="st">&quot;q4&quot;</span>, <span class="st">&quot;q5&quot;</span>, <span class="st">&quot;q6&quot;</span>, <span class="st">&quot;agelt35&quot;</span>,</span>
<span id="cb72-7"><a href="#cb72-7" aria-hidden="true" tabindex="-1"></a>                                                 <span class="st">&quot;agegt54&quot;</span>, <span class="st">&quot;durable&quot;</span>, <span class="st">&quot;lusd&quot;</span>,</span>
<span id="cb72-8"><a href="#cb72-8" aria-hidden="true" tabindex="-1"></a>                                                 <span class="st">&quot;husd&quot;</span>))</span>
<span id="cb72-9"><a href="#cb72-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Print data backend: Lists main attributes and methods of a DoubleMLData object</span></span>
<span id="cb72-10"><a href="#cb72-10" aria-hidden="true" tabindex="-1"></a>obj_dml_data_bonus</span></code></pre></div>
<p></p>
<p></p>
<div class="sourceCode" id="cb73"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb73-1"><a href="#cb73-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Print data set (output suppressed)</span></span>
<span id="cb73-2"><a href="#cb73-2" aria-hidden="true" tabindex="-1"></a>obj_dml_data_bonus<span class="sc">$</span>data</span></code></pre></div>
<pre><code>##       inuidur1 female black othrace dep1 dep2 q2 q3 q4 q5 q6 agelt35 agegt54
##    1: 2.890372      0     0       0    0    1  0  0  0  1  0       0       0
##    2: 0.000000      0     0       0    0    0  0  0  0  1  0       0       0
##    3: 3.295837      0     0       0    0    0  0  0  1  0  0       0       0
##    4: 2.197225      0     0       0    0    0  0  1  0  0  0       1       0
##    5: 3.295837      0     0       0    1    0  0  0  0  1  0       0       1
##   ---                                                                       
## 5095: 2.302585      0     0       0    0    0  0  1  0  0  0       1       0
## 5096: 1.386294      0     0       0    0    1  1  0  0  0  0       0       0
## 5097: 2.197225      0     0       0    0    1  1  0  0  0  0       1       0
## 5098: 1.386294      0     0       0    0    0  0  0  0  1  0       0       1
## 5099: 3.295837      0     0       0    0    0  0  0  1  0  0       0       1
##       durable lusd husd tg
##    1:       0    0    1  0
##    2:       0    1    0  0
##    3:       0    1    0  0
##    4:       0    0    0  1
##    5:       1    1    0  0
##   ---                     
## 5095:       0    0    0  1
## 5096:       0    0    0  1
## 5097:       0    1    0  0
## 5098:       0    0    0  1
## 5099:       1    1    0  0</code></pre>
<p></p>
<p></p>
<div class="sourceCode" id="cb75"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb75-1"><a href="#cb75-1" aria-hidden="true" tabindex="-1"></a>learner_classif_m <span class="ot">=</span> <span class="fu">lrn</span>(<span class="st">&quot;classif.ranger&quot;</span>, <span class="at">num.trees =</span> <span class="dv">500</span>,</span>
<span id="cb75-2"><a href="#cb75-2" aria-hidden="true" tabindex="-1"></a>                        <span class="at">min.node.size =</span> <span class="dv">2</span>, <span class="at">max.depth =</span> <span class="dv">5</span>)</span>
<span id="cb75-3"><a href="#cb75-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb75-4"><a href="#cb75-4" aria-hidden="true" tabindex="-1"></a>doubleml_irm_bonus <span class="ot">=</span> DoubleMLIRM<span class="sc">$</span><span class="fu">new</span>(obj_dml_data_bonus, </span>
<span id="cb75-5"><a href="#cb75-5" aria-hidden="true" tabindex="-1"></a>                                     <span class="at">ml_m =</span> learner_classif_m, </span>
<span id="cb75-6"><a href="#cb75-6" aria-hidden="true" tabindex="-1"></a>                                     <span class="at">ml_g =</span> learner_g, </span>
<span id="cb75-7"><a href="#cb75-7" aria-hidden="true" tabindex="-1"></a>                                     <span class="at">score =</span> <span class="st">&quot;ATE&quot;</span>,</span>
<span id="cb75-8"><a href="#cb75-8" aria-hidden="true" tabindex="-1"></a>                                     <span class="at">dml_procedure =</span> <span class="st">&quot;dml1&quot;</span>,</span>
<span id="cb75-9"><a href="#cb75-9" aria-hidden="true" tabindex="-1"></a>                                     <span class="at">n_folds =</span> <span class="dv">5</span>, </span>
<span id="cb75-10"><a href="#cb75-10" aria-hidden="true" tabindex="-1"></a>                                     <span class="at">n_rep =</span> <span class="dv">1</span>)</span>
<span id="cb75-11"><a href="#cb75-11" aria-hidden="true" tabindex="-1"></a><span class="co"># output suppressed</span></span>
<span id="cb75-12"><a href="#cb75-12" aria-hidden="true" tabindex="-1"></a>doubleml_irm_bonus</span></code></pre></div>
<pre><code>## ================= DoubleMLIRM Object ==================
## 
## 
## ------------------ Data summary      ------------------
## Outcome variable: inuidur1
## Treatment variable(s): tg
## Covariates: female, black, othrace, dep1, dep2, q2, q3, q4, q5, q6, agelt35, agegt54, durable, lusd, husd
## Instrument(s): 
## No. Observations: 5099
## 
## ------------------ Score &amp; algorithm ------------------
## Score function: ATE
## DML algorithm: dml1
## 
## ------------------ Machine learner   ------------------
## ml_g: regr.ranger
## ml_m: classif.ranger
## 
## ------------------ Resampling        ------------------
## No. folds: 5
## No. repeated sample splits: 1
## Apply cross-fitting: TRUE
## 
## ------------------ Fit summary       ------------------
## </code></pre>
<pre><code>## fit() not yet called.</code></pre>
<p></p>
<p></p>
<p></p>
<div class="sourceCode" id="cb78"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb78-1"><a href="#cb78-1" aria-hidden="true" tabindex="-1"></a>doubleml_data <span class="ot">=</span> <span class="fu">double_ml_data_from_data_frame</span>(df,</span>
<span id="cb78-2"><a href="#cb78-2" aria-hidden="true" tabindex="-1"></a>                                               <span class="at">y_col =</span> <span class="st">&quot;y&quot;</span>,</span>
<span id="cb78-3"><a href="#cb78-3" aria-hidden="true" tabindex="-1"></a>                                               <span class="at">d_cols =</span> <span class="fu">c</span>(<span class="st">&quot;X1&quot;</span>, <span class="st">&quot;X2&quot;</span>, <span class="st">&quot;X3&quot;</span>, </span>
<span id="cb78-4"><a href="#cb78-4" aria-hidden="true" tabindex="-1"></a>                                                          <span class="st">&quot;X4&quot;</span>, <span class="st">&quot;X5&quot;</span>, <span class="st">&quot;X6&quot;</span>, </span>
<span id="cb78-5"><a href="#cb78-5" aria-hidden="true" tabindex="-1"></a>                                                          <span class="st">&quot;X7&quot;</span>, <span class="st">&quot;X8&quot;</span>, <span class="st">&quot;X9&quot;</span>,</span>
<span id="cb78-6"><a href="#cb78-6" aria-hidden="true" tabindex="-1"></a>                                                          <span class="st">&quot;X10&quot;</span>))</span></code></pre></div>
<pre><code>## Set treatment variable d to X1.</code></pre>
<div class="sourceCode" id="cb80"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb80-1"><a href="#cb80-1" aria-hidden="true" tabindex="-1"></a><span class="co"># suppress output</span></span>
<span id="cb80-2"><a href="#cb80-2" aria-hidden="true" tabindex="-1"></a>doubleml_data</span></code></pre></div>
<pre><code>## ================= DoubleMLData Object ==================
## 
## 
## ------------------ Data summary      ------------------
## Outcome variable: y
## Treatment variable(s): X1, X2, X3, X4, X5, X6, X7, X8, X9, X10
## Covariates: X11, X12, X13, X14, X15, X16, X17, X18, X19, X20, X21, X22, X23, X24, X25, X26, X27, X28, X29, X30, X31, X32, X33, X34, X35, X36, X37, X38, X39, X40, X41, X42, X43, X44, X45, X46, X47, X48, X49, X50, X51, X52, X53, X54, X55, X56, X57, X58, X59, X60, X61, X62, X63, X64, X65, X66, X67, X68, X69, X70, X71, X72, X73, X74, X75, X76, X77, X78, X79, X80, X81, X82, X83, X84, X85, X86, X87, X88, X89, X90, X91, X92, X93, X94, X95, X96, X97, X98, X99, X100
## Instrument(s): 
## No. Observations: 500</code></pre>
<p></p>
<p></p>
<p></p>
<div class="sourceCode" id="cb82"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb82-1"><a href="#cb82-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Output: parameters after external tuning</span></span>
<span id="cb82-2"><a href="#cb82-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb82-3"><a href="#cb82-3" aria-hidden="true" tabindex="-1"></a><span class="co"># tuned parameters</span></span>
<span id="cb82-4"><a href="#cb82-4" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(doubleml_plr<span class="sc">$</span>params)</span></code></pre></div>
<p></p>
<p></p>
<pre><code>## List of 2
##  $ ml_g:List of 10
##   ..$ X1 :List of 1
##   .. ..$ lambda: num 0.09
##   ..$ X2 :List of 1
##   .. ..$ lambda: num 0.085
##   ..$ X3 : NULL
##   ..$ X4 : NULL
##   ..$ X5 : NULL
##   ..$ X6 : NULL
##   ..$ X7 : NULL
##   ..$ X8 : NULL
##   ..$ X9 : NULL
##   ..$ X10: NULL
##  $ ml_m:List of 10
##   ..$ X1 :List of 1
##   .. ..$ lambda: num 0.1
##   ..$ X2 :List of 1
##   .. ..$ lambda: num 0.095
##   ..$ X3 : NULL
##   ..$ X4 : NULL
##   ..$ X5 : NULL
##   ..$ X6 : NULL
##   ..$ X7 : NULL
##   ..$ X8 : NULL
##   ..$ X9 : NULL
##   ..$ X10: NULL</code></pre>
<p></p>
<p></p>
<p></p>
<div class="sourceCode" id="cb84"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb84-1"><a href="#cb84-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Output: parameters after internal tuning</span></span>
<span id="cb84-2"><a href="#cb84-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-3"><a href="#cb84-3" aria-hidden="true" tabindex="-1"></a><span class="co"># acces tuning results for target variable &quot;X1&quot;</span></span>
<span id="cb84-4"><a href="#cb84-4" aria-hidden="true" tabindex="-1"></a>doubleml_plr<span class="sc">$</span>tuning_res<span class="sc">$</span>X1</span></code></pre></div>
<p></p>
<p></p>
<pre><code>## $ml_g
## $ml_g[[1]]
## $ml_g[[1]]$tuning_result
## $ml_g[[1]]$tuning_result[[1]]
## $ml_g[[1]]$tuning_result[[1]]$tuning_result
##    lambda learner_param_vals  x_domain regr.mse
## 1:    0.1          &lt;list[2]&gt; &lt;list[1]&gt; 10.53451
## 
## $ml_g[[1]]$tuning_result[[1]]$tuning_archive
##     lambda regr.mse                                uhash  x_domain
##  1:  0.100 10.53451 a78ac75e-956e-4a38-ad3b-72951f2b649d &lt;list[1]&gt;
##  2:  0.095 10.60720 090902c3-b472-489e-8db2-2f0be5bdba05 &lt;list[1]&gt;
##  3:  0.085 10.76577 171c7b6a-2c9e-490d-bd89-51621ad02e75 &lt;list[1]&gt;
##  4:  0.055 11.32053 29ae301f-c659-41ff-8d7e-0a525a6616d7 &lt;list[1]&gt;
##  5:  0.060 11.21736 93e9dad9-241b-486a-a408-b7f0445aa76b &lt;list[1]&gt;
##  6:  0.050 11.42918 e4ae968f-733e-43a4-8adc-24cdbe1b49b2 &lt;list[1]&gt;
##  7:  0.075 10.93077 93b91b7f-74b6-4b8a-8f66-8cc6edd96743 &lt;list[1]&gt;
##  8:  0.065 11.11709 bf7b77e3-f20a-4984-9ed7-a0ae021a42a4 &lt;list[1]&gt;
##  9:  0.080 10.84518 dc3a215e-baec-45e2-a5fa-64c638dc3cba &lt;list[1]&gt;
## 10:  0.070 11.02168 5399e48c-0a80-47ee-b1b9-8a5c7fcc82fb &lt;list[1]&gt;
## 11:  0.090 10.68576 9c4cd6d9-2ceb-4e00-b4bf-cb211e005adb &lt;list[1]&gt;
##               timestamp batch_nr
##  1: 2021-06-03 11:37:19        1
##  2: 2021-06-03 11:37:19        2
##  3: 2021-06-03 11:37:20        3
##  4: 2021-06-03 11:37:20        4
##  5: 2021-06-03 11:37:20        5
##  6: 2021-06-03 11:37:20        6
##  7: 2021-06-03 11:37:20        7
##  8: 2021-06-03 11:37:21        8
##  9: 2021-06-03 11:37:21        9
## 10: 2021-06-03 11:37:21       10
## 11: 2021-06-03 11:37:21       11
## 
## $ml_g[[1]]$tuning_result[[1]]$params
## NULL
## 
## 
## 
## $ml_g[[1]]$params
## $ml_g[[1]]$params[[1]]
## $ml_g[[1]]$params[[1]]$family
## [1] &quot;gaussian&quot;
## 
## $ml_g[[1]]$params[[1]]$lambda
## [1] 0.1
## 
## 
## 
## 
## $ml_g$params
## $ml_g$params[[1]]
## $ml_g$params[[1]]$family
## [1] &quot;gaussian&quot;
## 
## $ml_g$params[[1]]$lambda
## [1] 0.1
## 
## 
## 
## 
## $ml_m
## $ml_m[[1]]
## $ml_m[[1]]$tuning_result
## $ml_m[[1]]$tuning_result[[1]]
## $ml_m[[1]]$tuning_result[[1]]$tuning_result
##    lambda learner_param_vals  x_domain  regr.mse
## 1:    0.1          &lt;list[2]&gt; &lt;list[1]&gt; 0.9794034
## 
## $ml_m[[1]]$tuning_result[[1]]$tuning_archive
##     lambda  regr.mse                                uhash  x_domain
##  1:  0.090 0.9798230 18e18030-ec9b-43d7-b0c5-6692438cbd09 &lt;list[1]&gt;
##  2:  0.055 0.9971462 31f787a2-208c-4ce9-8063-16a86d88e09e &lt;list[1]&gt;
##  3:  0.075 0.9830963 63864c9d-b4f4-4e5e-a24d-64daa8d106cb &lt;list[1]&gt;
##  4:  0.050 1.0045139 5b432aa2-f9c5-4347-a581-07ced999864a &lt;list[1]&gt;
##  5:  0.100 0.9794034 1706f85e-a9be-4a9f-94fb-fe6960ef396b &lt;list[1]&gt;
##  6:  0.060 0.9907519 43a491ef-deb4-4ab7-83da-ed3771202ca7 &lt;list[1]&gt;
##  7:  0.065 0.9869171 0ff0a00a-10b4-4596-a68c-4db6bef30640 &lt;list[1]&gt;
##  8:  0.095 0.9797396 9eda7a71-402e-4197-acf3-c56edbb6cc52 &lt;list[1]&gt;
##  9:  0.085 0.9804282 da741f14-f8c4-4695-84bf-63554cc66efc &lt;list[1]&gt;
## 10:  0.070 0.9848766 1ea1e377-e3ef-40a3-8859-66d9c41a6c1b &lt;list[1]&gt;
## 11:  0.080 0.9813190 f0502b9e-8779-445f-a68c-0f985118a1e4 &lt;list[1]&gt;
##               timestamp batch_nr
##  1: 2021-06-03 11:37:22        1
##  2: 2021-06-03 11:37:22        2
##  3: 2021-06-03 11:37:22        3
##  4: 2021-06-03 11:37:22        4
##  5: 2021-06-03 11:37:22        5
##  6: 2021-06-03 11:37:22        6
##  7: 2021-06-03 11:37:23        7
##  8: 2021-06-03 11:37:23        8
##  9: 2021-06-03 11:37:23        9
## 10: 2021-06-03 11:37:23       10
## 11: 2021-06-03 11:37:23       11
## 
## $ml_m[[1]]$tuning_result[[1]]$params
## NULL
## 
## 
## 
## $ml_m[[1]]$params
## $ml_m[[1]]$params[[1]]
## $ml_m[[1]]$params[[1]]$family
## [1] &quot;gaussian&quot;
## 
## $ml_m[[1]]$params[[1]]$lambda
## [1] 0.1
## 
## 
## 
## 
## $ml_m$params
## $ml_m$params[[1]]
## $ml_m$params[[1]]$family
## [1] &quot;gaussian&quot;
## 
## $ml_m$params[[1]]$lambda
## [1] 0.1</code></pre>
<p></p>
<p></p>
<div class="sourceCode" id="cb86"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb86-1"><a href="#cb86-1" aria-hidden="true" tabindex="-1"></a><span class="co"># tuned parameters</span></span>
<span id="cb86-2"><a href="#cb86-2" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(doubleml_plr<span class="sc">$</span>params)</span></code></pre></div>
<p></p>
<p></p>
<pre><code>## List of 2
##  $ ml_g:List of 10
##   ..$ X1 :List of 2
##   .. ..$ family: chr &quot;gaussian&quot;
##   .. ..$ lambda: num 0.1
##   ..$ X2 :List of 2
##   .. ..$ family: chr &quot;gaussian&quot;
##   .. ..$ lambda: num 0.1
##   ..$ X3 :List of 2
##   .. ..$ family: chr &quot;gaussian&quot;
##   .. ..$ lambda: num 0.1
##   ..$ X4 :List of 2
##   .. ..$ family: chr &quot;gaussian&quot;
##   .. ..$ lambda: num 0.09
##   ..$ X5 :List of 2
##   .. ..$ family: chr &quot;gaussian&quot;
##   .. ..$ lambda: num 0.07
##   ..$ X6 :List of 2
##   .. ..$ family: chr &quot;gaussian&quot;
##   .. ..$ lambda: num 0.085
##   ..$ X7 :List of 2
##   .. ..$ family: chr &quot;gaussian&quot;
##   .. ..$ lambda: num 0.085
##   ..$ X8 :List of 2
##   .. ..$ family: chr &quot;gaussian&quot;
##   .. ..$ lambda: num 0.08
##   ..$ X9 :List of 2
##   .. ..$ family: chr &quot;gaussian&quot;
##   .. ..$ lambda: num 0.09
##   ..$ X10:List of 2
##   .. ..$ family: chr &quot;gaussian&quot;
##   .. ..$ lambda: num 0.075
##  $ ml_m:List of 10
##   ..$ X1 :List of 2
##   .. ..$ family: chr &quot;gaussian&quot;
##   .. ..$ lambda: num 0.1
##   ..$ X2 :List of 2
##   .. ..$ family: chr &quot;gaussian&quot;
##   .. ..$ lambda: num 0.095
##   ..$ X3 :List of 2
##   .. ..$ family: chr &quot;gaussian&quot;
##   .. ..$ lambda: num 0.095
##   ..$ X4 :List of 2
##   .. ..$ family: chr &quot;gaussian&quot;
##   .. ..$ lambda: num 0.095
##   ..$ X5 :List of 2
##   .. ..$ family: chr &quot;gaussian&quot;
##   .. ..$ lambda: num 0.1
##   ..$ X6 :List of 2
##   .. ..$ family: chr &quot;gaussian&quot;
##   .. ..$ lambda: num 0.1
##   ..$ X7 :List of 2
##   .. ..$ family: chr &quot;gaussian&quot;
##   .. ..$ lambda: num 0.1
##   ..$ X8 :List of 2
##   .. ..$ family: chr &quot;gaussian&quot;
##   .. ..$ lambda: num 0.1
##   ..$ X9 :List of 2
##   .. ..$ family: chr &quot;gaussian&quot;
##   .. ..$ lambda: num 0.1
##   ..$ X10:List of 2
##   .. ..$ family: chr &quot;gaussian&quot;
##   .. ..$ lambda: num 0.1</code></pre>
<p></p>
<div style="page-break-after: always;"></div>
<p></p>
<p>The DGP is based on <span class="citation">Chernozhukov, Hansen, and Spindler (2015a)</span> and defined as <span class="math display">\[\begin{align}\begin{aligned}z_i &amp;= \Pi x_i + \zeta_i,\\d_i &amp;= x_i&#39; \gamma + z_i&#39; \delta + u_i,\\y_i &amp;= \alpha d_i + x_i&#39; \beta + \varepsilon_i,\end{aligned}\end{align}\]</span> with <span class="math display">\[\begin{align*}
\left(\begin{matrix} \varepsilon_i \\ u_i \\ \zeta_i \\ x_i \end{matrix} \right) \sim \mathcal{N}\left(0, \left(\begin{matrix} 1 &amp; 0.6 &amp; 0 &amp; 0 \\ 0.6 &amp; 1 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 0.25 I_{p_n^z} &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; \Sigma \end{matrix} \right) \right)
\end{align*}\]</span> where <span class="math inline">\(\Sigma\)</span> is a <span class="math inline">\(p^x_n \times p^x_n\)</span> matrix with entries <span class="math inline">\(\Sigma_{kj} = 0.5^{|k-j|}\)</span> and <span class="math inline">\(I_{p_n^z}\)</span> is an identity matrix with dimension <span class="math inline">\(p_n^z \times p_n^z\)</span>. <span class="math inline">\(\beta=\gamma\)</span> is a <span class="math inline">\(p^x_n\)</span>-vector with entries <span class="math inline">\(\beta=\frac{1}{j^2}\)</span> and <span class="math inline">\(\Pi = (I_{p_n^z}, 0_{p_n^z \times (p_n^x - p_n^z)})\)</span>. In the simulation example, we have one instrument, i.e., <span class="math inline">\(p^z_n=1\)</span> and <span class="math inline">\(p^x_n=20\)</span> regressors <span class="math inline">\(x_i\)</span>. In the simulation study, data sets with <span class="math inline">\(n = 500\)</span> observations are generated in <span class="math inline">\(R = 500\)</span> independent repetitions.</p>
<p></p>
<p>The DGP is based on a simulation study in <span class="citation">Belloni et al. (2017)</span> and defined as <span class="math display">\[\begin{align}
\begin{aligned}
d_i &amp;= 1\left\{ \frac{\exp(c_d x_i&#39; \beta)}{1+\exp(c_d x_i&#39; \beta)} &gt; v_i \right\}, &amp; &amp;v_i \sim \mathcal{U}(0,1),\\
y_i &amp;= \theta d_i + c_y x_i&#39; \beta d_i + \zeta_i, &amp; &amp;\zeta_i \sim \mathcal{N}(0,1),
\end{aligned}
\end{align}\]</span> with covariates <span class="math inline">\(x_i \sim \mathcal{N}(0, \Sigma)\)</span> where <span class="math inline">\(\Sigma\)</span> is a matrix with entries <span class="math inline">\(\Sigma_{kj} = 0.5^{|k-j|}\)</span>. <span class="math inline">\(\beta\)</span> is a <span class="math inline">\(p_x\)</span>-dimensional vector with entries <span class="math inline">\(\beta_j= \frac{1}{j^2}\)</span> and the constants <span class="math inline">\(c_y\)</span> and <span class="math inline">\(c_d\)</span> are determined as <span class="math display">\[\begin{align*}
c_y = \sqrt{\frac{R_y^2}{(1-R_y^2) \beta&#39; \Sigma \beta}}, \qquad c_d = \sqrt{\frac{(\pi^2 /3) R_d^2}{(1-R_d^2) \beta&#39; \Sigma \beta}}.
\end{align*}\]</span> We set the values of <span class="math inline">\(R_y=0.8\)</span> and <span class="math inline">\(R_d=0.8\)</span> and consider a setting with <span class="math inline">\(n=1000\)</span> and <span class="math inline">\(p=20\)</span>. Data generation and estimation have been performed in <span class="math inline">\(R = 500\)</span> independent replications.</p>
<p></p>
<p>The DGP is defined as <span class="math display">\[\begin{align}
\begin{aligned}d_i &amp;= 1\left\lbrace \alpha_x Z + v_i &gt; 0 \right\rbrace,\\y_i &amp;= \theta d_i + x_i&#39; \beta + u_i,
\end{aligned}
\end{align}\]</span> with <span class="math inline">\(Z\sim \text{Bernoulli}(0.5)\)</span> and <span class="math display">\[\begin{align*}
\left(\begin{matrix} u_i \\ v_i \end{matrix} \right) \sim \mathcal{N}\left(0, \left(\begin{matrix} 1 &amp; 0.3 \\ 0.3 &amp; 1 \end{matrix} \right) \right).
\end{align*}\]</span> The covariates are drawn from a multivariate normal distribution with <span class="math inline">\(x_i\sim \mathcal{N}(0, \Sigma)\)</span> with entries of the matrix <span class="math inline">\(\Sigma\)</span> being <span class="math inline">\(\Sigma_{kj} = 0.5^{|j-k|}\)</span> and <span class="math inline">\(\beta\)</span> being a <span class="math inline">\(p_x\)</span>-dimensional vector with <span class="math inline">\(\beta_j=\frac{1}{\beta^2}\)</span>. The data generating process is inspired by a process used in a simulation in <span class="citation">Farbmacher, Guber, and Klaassen (2020)</span>. In the simulation study, data sets with <span class="math inline">\(n = 1000\)</span> observations and <span class="math inline">\(p_x = 20\)</span> confounding variables <span class="math inline">\(x_i\)</span> have been generated in <span class="math inline">\(R = 500\)</span> independent repetitions.</p>
<div style="page-break-after: always;"></div>
<div id="references" class="section level1 unnumbered">
<h1 class="unnumbered">References</h1>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-grfpaper" class="csl-entry">
Athey, Susan, Julie Tibshirani, and Stefan Wager. 2019. <span>“Generalized Random Forests.”</span> <em>The Annals of Statistics</em> 47 (2): 1148–78. <a href="https://cran.r-project.org/package=grf">https://cran.r-project.org/package=grf</a>.
</div>
<div id="ref-DoubleMLpython" class="csl-entry">
Bach, Philipp, Victor Chernozhukov, Malte S Kurz, and Martin Spindler. 2021. <span>“<span>DoubleML</span> – <span>A</span>n Object-Oriented Implementation of Double Machine Learning in <span>P</span>ython.”</span> <em>arXiv Preprint arXiv:2104.03220</em>.
</div>
<div id="ref-bach2018valid" class="csl-entry">
Bach, Philipp, Victor Chernozhukov, and Martin Spindler. 2018. <span>“Valid Simultaneous Inference in High-Dimensional Settings (with the Hdm Package for r).”</span> <em>arXiv Preprint arXiv:1809.04951</em>.
</div>
<div id="ref-mlr3book" class="csl-entry">
Becker, Marc, Martin Binder, Bernd Bischl, Michel Lang, Florian Pfisterer, Nicholas G. Reich, Jakob Richter, Patrick Schratz, and Raphael Sonabend. 2021. <span>“Mlr3 Book.”</span> <a href="https://mlr3book.mlr-org.com">https://mlr3book.mlr-org.com</a>.
</div>
<div id="ref-mlr3tuning" class="csl-entry">
Becker, Marc, Michel Lang, Jakob Richter, Bernd Bischl, and Daniel Schalk. 2020. <em>Mlr3tuning: Tuning for ’Mlr3’</em>. <a href="https://CRAN.R-project.org/package=mlr3tuning">https://CRAN.R-project.org/package=mlr3tuning</a>.
</div>
<div id="ref-belloni2012sparse" class="csl-entry">
Belloni, Alexandre, Daniel Chen, Victor Chernozhukov, and Christian Hansen. 2012. <span>“Sparse Models and Methods for Optimal Instruments with an Application to Eminent Domain.”</span> <em>Econometrica</em> 80: 2369–429.
</div>
<div id="ref-zestim" class="csl-entry">
Belloni, Alexandre, Victor Chernozhukov, Denis Chetverikov, and Ying Wei. 2018. <span>“Uniformly Valid Post-Regularization Confidence Regions for Many Functional Parameters in z-Estimation Framework.”</span> <em>Annals of Statistics</em> 46 (6B): 3643–75.
</div>
<div id="ref-belloni2017program" class="csl-entry">
Belloni, Alexandre, Victor Chernozhukov, Ivan Fernández-Val, and Christian Hansen. 2017. <span>“Program Evaluation and Causal Inference with High-Dimensional Data.”</span> <em>Econometrica</em> 85 (1): 233–98.
</div>
<div id="ref-belloni2011" class="csl-entry">
Belloni, Alexandre, Victor Chernozhukov, and Christian Hansen. 2011. <span>“Inference for High-Dimensional Sparse Econometric Models.”</span> In <em>Advances in Economics and Econometrics. 10th World Congress of Econometric Society. August 2010.</em>, III:245–295.
</div>
<div id="ref-bck2014" class="csl-entry">
Belloni, Alexandre, Victor Chernozhukov, and Kengo Kato. 2014. <span>“Uniform Post-Selection Inference for Least Absolute Deviation Regression and Other z-Estimation Problems.”</span> <em>Biometrika</em> 102 (1): 77–94. <a href="http://dx.doi.org/10.1093/biomet/asu056">http://dx.doi.org/10.1093/biomet/asu056</a>.
</div>
<div id="ref-belloni2014pivotal" class="csl-entry">
Belloni, Alexandre, Victor Chernozhukov, and Lie Wang. 2014. <span>“Pivotal Estimation via Square-Root Lasso in Nonparametric Regression.”</span> <em>The Annals of Statistics</em> 42 (2): 757–88.
</div>
<div id="ref-restud" class="csl-entry">
Belloni, Alexandre, Victor Chernozukov, and Christian Hansen. 2014. <span>“Inference on Treatment Effects After Selection Among High-Dimensional Controls.”</span> <em>The Review of Economic Studies</em> 81 (2 (287)): 608–50. <a href="http://www.jstor.org/stable/43551575">http://www.jstor.org/stable/43551575</a>.
</div>
<div id="ref-bickel1993efficient" class="csl-entry">
Bickel, Peter J, Chris AJ Klaassen, Ya’acov Ritov, and Jon A Wellner. 1993. <em>Efficient and Adaptive Estimation for Semiparametric Models</em>. Vol. 4. Johns Hopkins University Press Baltimore.
</div>
<div id="ref-bilias2000" class="csl-entry">
Bilias, Yannis. 2000. <span>“Sequential Testing of Duration Data: The Case of the <span>Pennsylvania</span> <span>‘Reemployment Bonus’</span> Experiment.”</span> <em>Journal of Applied Econometrics</em> 15 (6): 575–94.
</div>
<div id="ref-causalweight" class="csl-entry">
Bodory, Hugo, and Martin Huber. 2020. <em>Causalweight: Estimation Methods for Causal Inference Based on Inverse Probability Weighting</em>. <a href="https://CRAN.R-project.org/package=causalweight">https://CRAN.R-project.org/package=causalweight</a>.
</div>
<div id="ref-buhlmann2015high" class="csl-entry">
Bühlmann, Peter, and Sara van de Geer. 2015. <span>“High-Dimensional Inference in Misspecified Linear Models.”</span> <em>Electronic Journal of Statistics</em> 9 (1): 1449–73.
</div>
<div id="ref-R6" class="csl-entry">
Chang, Winston. 2020. <em>R6: Encapsulated Classes with Reference Semantics</em>. <a href="https://CRAN.R-project.org/package=R6">https://CRAN.R-project.org/package=R6</a>.
</div>
<div id="ref-dml2018" class="csl-entry">
Chernozhukov, Victor, Denis Chetverikov, Mert Demirer, Esther Duflo, Christian Hansen, Whitney Newey, and James Robins. 2018. <span>“Double/Debiased Machine Learning for Treatment and Structural Parameters.”</span> <em>The Econometrics Journal</em> 21 (1): C1–68. <a href="https://onlinelibrary.wiley.com/doi/abs/10.1111/ectj.12097">https://onlinelibrary.wiley.com/doi/abs/10.1111/ectj.12097</a>.
</div>
<div id="ref-cck2013" class="csl-entry">
Chernozhukov, Victor, Denis Chetverikov, and Kengo Kato. 2013. <span>“Gaussian Approximations and Multiplier Bootstrap for Maxima of Sums of High-Dimensional Random Vectors.”</span> <em>The Annals of Statistics</em> 41 (6): 2786–2819.
</div>
<div id="ref-cck2014" class="csl-entry">
———. 2014. <span>“Gaussian Approximation of Suprema of Empirical Processes.”</span> <em>The Annals of Statistics</em> 42 (4): 1564–97.
</div>
<div id="ref-hdm" class="csl-entry">
Chernozhukov, Victor, Chris Hansen, and Martin Spindler. 2016. <span>“<span class="nocase">hdm: High-Dimensional Metrics</span>.”</span> <em><span>The R Journal</span></em> 8 (2): 185–99. <a href="https://CRAN.R-project.org/package=hdm">https://CRAN.R-project.org/package=hdm</a>.
</div>
<div id="ref-CHSAERpp" class="csl-entry">
Chernozhukov, Victor, Christian Hansen, and Martin Spindler. 2015a. <span>“Post-Selection and Post-Regularization Inference in Linear Models with Many Controls and Instruments.”</span> <em>American Economic Review</em> 105 (5): 486–90.
</div>
<div id="ref-chernozhukov2015valid" class="csl-entry">
———. 2015b. <span>“Valid Post-Selection and Post-Regularization Inference: An Elementary, General Approach.”</span> <em>Annual Review of Economics</em> 7 (1): 649–88. <a href="https://doi.org/10.1146/annurev-economics-012315-015826">https://doi.org/10.1146/annurev-economics-012315-015826</a>.
</div>
<div id="ref-hdi" class="csl-entry">
Dezeure, Ruben, Peter Bühlmann, Lukas Meier, and Nicolai Meinshausen. 2015. <span>“High-Dimensional Inference: Confidence Intervals, p-Values and <span>R</span>-Software <span class="nocase">hdi</span>.”</span> <em>Statistical Science</em> 30 (4): 533–58.
</div>
<div id="ref-datatable" class="csl-entry">
Dowle, Matt, and Arun Srinivasan. 2020. <em>Data.table: Extension of ‘Data.frame‘</em>. <a href="https://CRAN.R-project.org/package=data.table">https://CRAN.R-project.org/package=data.table</a>.
</div>
<div id="ref-latest" class="csl-entry">
Farbmacher, Helmut, Raphael Guber, and Sven Klaassen. 2020. <span>“Instrument Validity Tests with Causal Forests.”</span> <em>Journal of Business &amp; Economic Statistics</em>, 1–10.
</div>
<div id="ref-glmnet" class="csl-entry">
Friedman, Jerome, Trevor Hastie, and Robert Tibshirani. 2010. <span>“Regularization Paths for Generalized Linear Models via Coordinate Descent.”</span> <em>Journal of Statistical Software</em> 33(1): 1–22. <a href="http://www.jstatsoft.org/v33/i01">http://www.jstatsoft.org/v33/i01</a>.
</div>
<div id="ref-LATE" class="csl-entry">
Imbens, Guido W., and Joshua D. Angrist. 1994. <span>“Identification and Estimation of Local Average Treatment Effects.”</span> <em>Econometrica</em> 62 (2): 467–75. <a href="http://www.jstor.org/stable/2951620">http://www.jstor.org/stable/2951620</a>.
</div>
<div id="ref-javanmard2014hypothesis" class="csl-entry">
Javanmard, Adel, and Andrea Montanari. 2014. <span>“Hypothesis Testing in High-Dimensional Regression Under the Gaussian Random Design Model: Asymptotic Theory.”</span> <em>IEEE Transactions on Information Theory</em> 60 (10): 6522–54.
</div>
<div id="ref-javanmard2018debiasing" class="csl-entry">
———. 2018. <span>“Debiasing the Lasso: Optimal Sample Size for Gaussian Designs.”</span> <em>The Annals of Statistics</em> 46 (6A): 2593–2622.
</div>
<div id="ref-knaus2020" class="csl-entry">
Knaus, Michael C. 2020. <span>“Double Machine Learning Based Program Evaluation Under Unconfoundedness.”</span> <em>arXiv Preprint arXiv:2003.03191</em>. <a href="https://github.com/MCKnaus/causalDML">https://github.com/MCKnaus/causalDML</a>.
</div>
<div id="ref-knaus2018" class="csl-entry">
———. 2021. <span>“A Double Machine Learning Approach to Estimate the Effects of Musical Practice on Student’s Skills.”</span> <em>Journal of the Royal Statistical Society: Series A (Statistics in Society)</em> 184 (1): 282–300. <a href="https://github.com/MCKnaus/dmlmt">https://github.com/MCKnaus/dmlmt</a>.
</div>
<div id="ref-doubleml_serverless" class="csl-entry">
Kurz, Malte S. 2021. <span>“Distributed Double Machine Learning with a Serverless Architecture.”</span> In <em>Companion of the ACM/SPEC International Conference on Performance Engineering</em>, 27–33. ICPE ’21. New York, NY, USA: Association for Computing Machinery. <a href="https://doi.org/10.1145/3447545.3451181">https://doi.org/10.1145/3447545.3451181</a>.
</div>
<div id="ref-mlr3learners" class="csl-entry">
Lang, Michel, Quay Au, Stefan Coors, and Patrick Schratz. 2020. <em>Mlr3learners: Recommended Learners for ’Mlr3’</em>. <a href="https://CRAN.R-project.org/package=mlr3learners">https://CRAN.R-project.org/package=mlr3learners</a>.
</div>
<div id="ref-mlr3" class="csl-entry">
Lang, Michel, Martin Binder, Jakob Richter, Patrick Schratz, Florian Pfisterer, Stefan Coors, Quay Au, Giuseppe Casalicchio, Lars Kotthoff, and Bernd Bischl. 2019. <span>“<span class="nocase">mlr3</span>: A Modern Object-Oriented Machine Learning Framework in <span>R</span>.”</span> <em>Journal of Open Source Software</em>. <a href="https://joss.theoj.org/papers/10.21105/joss.01903">https://joss.theoj.org/papers/10.21105/joss.01903</a>.
</div>
<div id="ref-paradox" class="csl-entry">
Lang, Michel, Bernd Bischl, Jakob Richter, Xudong Sun, and Martin Binder. 2020. <em>Paradox: Define and Work with Parameter Spaces for Complex Algorithms</em>. <a href="https://CRAN.R-project.org/package=paradox">https://CRAN.R-project.org/package=paradox</a>.
</div>
<div id="ref-econml" class="csl-entry">
Microsoft Research. 2019. <span>“<span>EconML</span>: <span class="nocase">A Python package for ML-based heterogeneous treatment effects estimation</span>.”</span> https://github.com/microsoft/EconML.
</div>
<div id="ref-newey1994asymptotic" class="csl-entry">
Newey, Whitney. 1994. <span>“The Asymptotic Variance of Semiparametric Estimators.”</span> <em>Econometrica</em> 62 (6): 1349–82.
</div>
<div id="ref-neyman" class="csl-entry">
Neyman, Jerzy. 1959. <span>“Optimal Asymptotic Tests of Composite Hypotheses.”</span> In <em>Probability and Statistics</em>, edited by Ulf Grenander, 213–34. Almqvist &amp; Wiksell.
</div>
<div id="ref-pedregosa2011" class="csl-entry">
Pedregosa, Fabian, Gaël Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, et al. 2011. <span>“Scikit-Learn: Machine Learning in Python.”</span> <em>Journal of Machine Learning Research</em> 12 (85): 2825–30. <a href="http://jmlr.org/papers/v12/pedregosa11a.html">http://jmlr.org/papers/v12/pedregosa11a.html</a>.
</div>
<div id="ref-permutt1989" class="csl-entry">
Permutt, Thomas, and J Richard Hebel. 1989. <span>“Simultaneous-Equation Estimation in a Clinical Trial of the Effect of Smoking on Birth Weight.”</span> <em>Biometrics</em> 45: 619–22.
</div>
<div id="ref-R" class="csl-entry">
R Core Team. 2020. <em>R: A Language and Environment for Statistical Computing</em>. Vienna, Austria: R Foundation for Statistical Computing. <a href="https://www.R-project.org/">https://www.R-project.org/</a>.
</div>
<div id="ref-robins1995" class="csl-entry">
Robins, James M, and Andrea Rotnitzky. 1995. <span>“Semiparametric Efficiency in Multivariate Regression Models with Missing Data.”</span> <em>Journal of the American Statistical Association</em> 90 (429): 122–29.
</div>
<div id="ref-robinson1988" class="csl-entry">
Robinson, Peter M. 1988. <span>“Root-n-Consistent Semiparametric Regression.”</span> <em>Econometrica</em> 56 (4): 931–54.
</div>
<div id="ref-rw2005" class="csl-entry">
Romano, Joseph P, and Michael Wolf. 2005a. <span>“Exact and Approximate Stepdown Methods for Multiple Hypothesis Testing.”</span> <em>Journal of the American Statistical Association</em> 100 (469): 94–108.
</div>
<div id="ref-rw22005" class="csl-entry">
———. 2005b. <span>“Stepwise Multiple Testing as Formalized Data Snooping.”</span> <em>Econometrica</em> 73 (4): 1237–82.
</div>
<div id="ref-rw2016" class="csl-entry">
———. 2016. <span>“Efficient Computation of Adjusted p-Values for Resampling-Based Stepdown Multiple Testing.”</span> <em>Statistics &amp; Probability Letters</em> 113: 38–40.
</div>
<div id="ref-schick1986" class="csl-entry">
Schick, Anton. 1986. <span>“On Asymptotically Efficient Estimation in Semiparametric Models.”</span> <em>The Annals of Statistics</em> 14 (3): 1139–51.
</div>
<div id="ref-mlr3extralearners" class="csl-entry">
Sonabend, Raphael, and Patrick Schratz. 2020. <em>Mlr3extralearners: Extra Learners for Mlr3</em>.
</div>
<div id="ref-postDoubleR" class="csl-entry">
Szitas, Juraj. 2019. <span>“<span class="nocase">postDoubleR</span>: Post Double Selection with Double Machine Learning.”</span> <a href="https://CRAN.R-project.org/package=postDoubleR">https://CRAN.R-project.org/package=postDoubleR</a>.
</div>
<div id="ref-grf" class="csl-entry">
Tibshirani, Julie, Susan Athey, and Stefan Wager. 2020. <em>Grf: Generalized Random Forests</em>. <a href="https://CRAN.R-project.org/package=grf">https://CRAN.R-project.org/package=grf</a>.
</div>
<div id="ref-van2014asymptotically" class="csl-entry">
van de Geer, Sara, Peter Bühlmann, Ya’acov Ritov, and Ruben Dezeure. 2014. <span>“On Asymptotically Optimal Confidence Regions and Tests for High-Dimensional Models.”</span> <em>Annals of Statistics</em> 42 (3): 1166–1202. <a href="https://doi.org/10.1214/14-AOS1221">https://doi.org/10.1214/14-AOS1221</a>.
</div>
<div id="ref-van2011targeted" class="csl-entry">
Van der Laan, Mark J, and Sherri Rose. 2011. <em>Targeted Learning: Causal Inference for Observational and Experimental Data</em>. Springer Science &amp; Business Media.
</div>
<div id="ref-van2000asymptotic" class="csl-entry">
Van der Vaart, Aad W. 2000. <em>Asymptotic Statistics</em>. Vol. 3. Cambridge university press.
</div>
<div id="ref-advr" class="csl-entry">
Wickham, Hadley. 2019. <em>Advanced <span>R</span></em>. CRC press.
</div>
<div id="ref-ranger" class="csl-entry">
Wright, Marvin N., and Andreas Ziegler. 2017. <span>“<span class="nocase">ranger</span>: A Fast Implementation of Random Forests for High Dimensional Data in <span>C++</span> and <span>R</span>.”</span> <em>Journal of Statistical Software</em> 77 (1): 1–17. <a href="https://doi.org/10.18637/jss.v077.i01">https://doi.org/10.18637/jss.v077.i01</a>.
</div>
<div id="ref-wright1928" class="csl-entry">
Wright, Philip G. 1928. <em>Tariff on Animal and Vegetable Oils</em>. Macmillan Company, New York.
</div>
<div id="ref-zhang2014" class="csl-entry">
Zhang, Cun-Hui, and Stephanie S. Zhang. 2014. <span>“Confidence Intervals for Low Dimensional Parameters in High Dimensional Linear Models.”</span> <em>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</em> 76 (1): 217–42. <a href="https://doi.org/10.1111/rssb.12026">https://doi.org/10.1111/rssb.12026</a>.
</div>
<div id="ref-AIPW" class="csl-entry">
Zhong, Yongqi, and Ashley Naimi. 2021. <em>AIPW: Augmented Inverse Probability Weighting (AIPW) for Binary Exposure</em>. <a href="https://github.com/yqzhong7/AIPW">https://github.com/yqzhong7/AIPW</a>.
</div>
</div>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>GitHub repository for R package: <a href="https://github.com/DoubleML/doubleml-for-r" class="uri">https://github.com/DoubleML/doubleml-for-r</a>.<a href="#fnref1" class="footnote-back">↩︎</a></p></li>
<li id="fn2"><p><a href="https://docs.doubleml.org/stable/index.html" class="uri">https://docs.doubleml.org/stable/index.html</a><a href="#fnref2" class="footnote-back">↩︎</a></p></li>
<li id="fn3"><p><a href="https://docs.doubleml.org/r/stable/" class="uri">https://docs.doubleml.org/r/stable/</a><a href="#fnref3" class="footnote-back">↩︎</a></p></li>
<li id="fn4"><p><a href="https://r6.r-lib.org/articles/" class="uri">https://r6.r-lib.org/articles/</a><a href="#fnref4" class="footnote-back">↩︎</a></p></li>
<li id="fn5"><p>The resulting grid has 11 equally spaced values ranging from a minimum value of 0.05 to a maximum value of 0.1. Type <code>generate_design_grid(par_grids$ml_g, resolution = 11)</code> to access the grid for nuisance part <code>ml_g</code>.<a href="#fnref5" class="footnote-back">↩︎</a></p></li>
</ol>
</div>



<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
